<!DOCTYPE html>













<html class="theme-next gemini" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">

<meta name="google-site-verification" content="jgw73iXouBAJcOuff0yi9vdSNDecBSOUXacsHJszpmo" />
<meta name="baidu-site-verification" content="xyf9WD2vvl" />











<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/apple-icon-57x57.png?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_body":"slideDownIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="论文: FCOS: Fully Convolutional One-Stage Object Detection作者: Zhi Tian Chunhua Shen Hao Chen Tong He机构: The University of Adelaide, Australia Abstract我们提出了一种全卷积的 one-stage 目标检测器（FCOS, Fully Convolutiona">
<meta name="keywords" content="目标检测,计算机视觉,论文解读">
<meta property="og:type" content="article">
<meta property="og:title" content="FCOS (CVPR, 2019)">
<meta property="og:url" content="https://hellozhaozheng.github.io/z_post/计算机视觉-FCOS-CVPR2019/index.html">
<meta property="og:site_name" content="从零开始的BLOG">
<meta property="og:description" content="论文: FCOS: Fully Convolutional One-Stage Object Detection作者: Zhi Tian Chunhua Shen Hao Chen Tong He机构: The University of Adelaide, Australia Abstract我们提出了一种全卷积的 one-stage 目标检测器（FCOS, Fully Convolutiona">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab5.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab6.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab7.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab8.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig5.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig6.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab9.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig7.jpg?x-oss-process=style/blog_img">
<meta property="og:updated_time" content="2019-07-19T08:29:09.210Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FCOS (CVPR, 2019)">
<meta name="twitter:description" content="论文: FCOS: Fully Convolutional One-Stage Object Detection作者: Zhi Tian Chunhua Shen Hao Chen Tong He机构: The University of Adelaide, Australia Abstract我们提出了一种全卷积的 one-stage 目标检测器（FCOS, Fully Convolutiona">
<meta name="twitter:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig1.jpg?x-oss-process=style/blog_img">






  <link rel="canonical" href="https://hellozhaozheng.github.io/z_post/计算机视觉-FCOS-CVPR2019/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>FCOS (CVPR, 2019) | 从零开始的BLOG</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?21a4899cc63d3c11a3d90ac58074a19c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">从零开始的BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">与其感慨路难行，不如马上出发</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档<span class="badge">268</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-计算机视觉">
    <a href="/categories/计算机视觉/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tripadvisor"></i> <br />计算机视觉</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-深度学习">
    <a href="/categories/深度学习/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-drupal"></i> <br />深度学习</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-caffe2">
    <a href="/categories/Caffe2/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-coffee"></i> <br />Caffe2</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-pytorch">
    <a href="/categories/PyTorch/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-free-code-camp"></i> <br />PyTorch</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-c++">
    <a href="/categories/Cpp/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-codiepie"></i> <br />C++</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-python">
    <a href="/categories/Python/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-product-hunt"></i> <br />Python</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-项目">
    <a href="/categories/项目/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-connectdevelop"></i> <br />项目</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-cuda">
    <a href="/categories/CUDA/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-braille"></i> <br />CUDA</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-其他">
    <a href="/categories/其他/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />其他</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签<span class="badge">41</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于我</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />站内搜索(首次加载需3~5秒)</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="站内搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  
  
    
      
    
    <a href="https://github.com/hellozhaozheng" class="github-corner" target="_blank" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#222; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg>
    
      </a>
    



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://hellozhaozheng.github.io/z_post/计算机视觉-FCOS-CVPR2019/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZeroZone">
      <meta itemprop="description" content="吾乃闪耀的芝士蛋挞!">
      <meta itemprop="image" content="/images/avatar_zz.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="从零开始的BLOG">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">FCOS (CVPR, 2019)
              
            
          </h1>
        

        <div class="post-meta">
	
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-12 21:13:35" itemprop="dateCreated datePublished" datetime="2019-05-12T21:13:35+08:00">2019-05-12</time>
            

            
          </span>

	  
  	    <span class="post-updated">
    		&nbsp; | &nbsp; 更新于
    		<time itemprop="dateUpdated" datetime="2019-07-19T16:29:09+08:00" content="2019-07-19">
      		  2019-07-19
    		</time>
  	  </span>
	  

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/计算机视觉/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/z_post/计算机视觉-FCOS-CVPR2019/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/z_post/计算机视觉-FCOS-CVPR2019/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">15k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">14 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>论文:</strong> FCOS: Fully Convolutional One-Stage Object Detection<br><strong>作者:</strong> Zhi Tian Chunhua Shen Hao Chen Tong He<br><strong>机构:</strong> The University of Adelaide, Australia</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了一种全卷积的 one-stage 目标检测器（FCOS, Fully Convolutional One Stage）, 它以对每个像素进行预测的方式来解决目标检测问题，<strong>类似于语义分割</strong>。几乎所有的 SOTA 物体检测器，如 RetinaNet，SSD，YOLOv3 和 Faster R-CNN 都依赖于预定义的 anchor box。相比之下，我们提出的 FCOS 不需要 anchor box，同时也不需要 proposals (即 One-Stage)。通过消除对预定义 anchor 的依赖，FCOS 完全避免了与 anchor box 相关的复杂计算，例如在训练期间计算 overlapping 并显着减少 training memory footprint。更重要的是，我们还避免了与 anchor 相关的所有超参数，<strong>这些参数通常对最终检测性能非常敏感</strong>。凭借唯一的后处理操作非最大抑制（NMS），我们的 FCOS 优于之前的 anchor-based one-stage detectors，并且结构更简单。我们首次展示了一种更加简单灵活的检测框架，可以提高检测精度。我们希望 FCOS 框架可以作为许多其他实例级任务简单而强大的替代方案。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>对象检测是计算机视觉中的基本但具有挑战性的任务，其要求算法针对图像中的每个感兴趣实例预测出具有类别标签的边界框。 目前所有主流的探测器，如Faster R-CNN，SSD 和YOLOv2，v3 等都依赖于一组预定义的 anchor box，<strong>长期以来人们一直认为 anchor 的使用是检测器成功的关键。</strong> 尽管取得了巨大成功，<strong>但重要的是要注意 anchor-based detectors 存在的一些缺点:</strong></p>
<ol>
<li>如 Faster R-CNN 和 Focal Loss 所示，检测性能对于尺寸，宽高比和 anchor 数量非常敏感。 例如，在RetinaNet 中，根据 COCO 的 benchmark 上，仅仅改变这些超参数就会影响AP的性能提升4％[13]。 因此，对于 anchor-based 检测器需要仔细调整这些超参数。</li>
<li>即使经过精心设计，由于 anchor box 的比例和宽高比保持固定，detectors 在处理具有较大形状变化的物体集合时会遇到困难，特别是对于小物体。 <strong>预定义的 anchor box 也妨碍了探测器的泛化能力</strong>，因为它们需要在具有不同物体尺寸或宽高比的新探测任务上进行重新设定.</li>
<li>为了实现高召回率，anchor-based 检测器需要将 anchor box 密集地放置在输入图像上（例如，在特征金字塔网络(FPN)中, 对于短边像素为 800 的输入图像, 会产生超过 180K 的 anchor box）。 大多数这些 anchor box 在训练期间会被标记为 negative samples。 过多的 negative samples 加剧了 training 过程中正负样本之间的不平衡性。</li>
<li>当在训练期间计算所有 anchor box 和 GT box 之间的 IOU 时，过多数量的 anchor box 也显著增加了计算量和存储器占用量。</li>
</ol>
<p>最近，完全卷积网络（FCN）[16]在密集预测任务中取得了巨大成功，如语义分割[16]，深度估计[14]，关键点检测[2]和计数[1]。 作为高级视觉任务之一，对象检测可能是唯一一个不使用逐像素预测框架的任务, 这主要是因为使用了 anchor box。 那么, 很自然地就会提出一个问题: <strong>我们能否以纯粹的每像素预测方式解决目标检测问题，类似于用 FCN 进行语义分割？</strong> 因此，这些基本视觉任务就可以在(几乎)一个单一框架中被统一起来。 我们认为答案是可以的。此外，我们还首次证明了基于 FCN(much simpler) 的检测器比基于 anchor 的检测器具有更好的性能</p>
<p>在文献中，一些工作试图利用基于fcn的框架来检测对象，如DenseBox[9]和UnitBox[24]。具体地说，这些基于fcn的框架直接预测了特征映射级别上每个空间位置上的一个4D向量加上一个类类别。如图1(左)所示，4D向量描述了一个边界框到该位置的四个边的相对偏移量。这些框架类似于用于语义分割的FCNs，只是每个位置都需要返回一个4D连续向量。但是，为了处理不同大小的边界框，DenseBox[9]将训练图像调整为固定的比例。因此，<strong>DenseBox必须对图像金字塔进行检测</strong>，这与FCN一次计算所有卷积的思想是相悖的。此外，更重要的是，这些方法主要用于特殊领域的目标检测，如场景文本检测[25]或人脸检测[24,9]，因为人们认为这些方法不适用于具有高度重叠边界框的通用目标检测。如图1(右)所示，高度重叠的边界框在训练过程中造成了难以处理的歧义: 对于重叠区域的像素，不清楚应该返回哪个边界框。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig1.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig1.jpg"></div></p>
<p>在接下来，我们将进一步研究这个问题，并说明使用FPN可以在很大程度上消除这种模糊性。结果表明，该方法与传统的基于 anchor 的检测方法具有相当的检测精度。此外，<strong>我们注意到我们的方法可能会在远离目标对象中心的位置产生大量低质量的预测边界框。</strong> 为了抑制这些低质量的检测结果, 我们引入一个新的 “center-ness” 分支(只有一层)来预测像素与对应边界框中心的偏差, 具体定义如公式(3)所示. 然后，该分数用于降低低质量检测边界框的权重，并将检测结果合并到NMS中。 这种简单而有效的中心分支使得基于 FCN 的探测器在完全相同的训练和测试设置下优于相应的 anchor-based detectors.</p>
<p>这种新的检测框架具有以下优点:</p>
<ol>
<li>检测任务现在与许多其他 FCN 可解决的任务（例如语义分割）相统一，从而可以更轻松地重复使用这些任务中的想法。</li>
<li>检测变为 proposal free 和 anchor free，这显著减少了超参数的数量。 超参数通常需要启发式调整，并且涉及许多技巧才能获得良好的性能。 而我们的新检测框架使检测器，特别是使它的 training 阶段变得相当简单。 此外，通过消除 anchor box，我们的新探测器完全避免了复杂的 IOU 计算以及训练期间 anchor box 和 GT box 之间的匹配，并将总的训练内存占用(training memory footprint)减少了2倍左右。</li>
<li>Without bells and whistles，我们在 One-Stage Detectors 中实现了 SOTA 的结果。 我们的实验还表明，<strong>本文所提出的 FCOS 可以用作 Two-Stage Detectors 中的 RPN</strong>，并且可以实现比基于 anchor 的 RPN 更好的性能。 <strong>鉴于更简单的 anchor free Detectors 具有更好的性能，我们鼓励大家重新考虑物体检测中 anchor 的必要性，虽然目前这被认为是检测任务的事实标准(defacto standard for detection)。</strong></li>
<li>我们所提出的 detector 只需做很小的修改就可以立即扩展到其他视觉任务，包括实例分割和关键点检测。 我们相信这种新方法可以成为许多实例级预测问题的新 baseline。</li>
</ol>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><p>Anchor-based Detectors: R-CNN, RPN, SSD, YOLOv2/v3</p>
<p>Anchor-free Detectors: YOLOv1, CornerNet, DenseBox<br>最流行的 anchor free 探测器可能是YOLOv1[17]。YOLOv1没有使用锚框，而是预测在靠近对象中心的点上的边界框。只使用中心附近的点，因为它们被认为能够产生更高质量的检测。<strong>然而，由于仅使用靠近中心的点来预测边界框，YOLOv1的召回率较低，正如YOLOv2[18]中所述。</strong> 因此，YOLOv2[18]也使用了锚盒。与YOLOv1相比，FCOS利用 GT bounding box 中的所有点来预测 bounding boxes，并通过提出的 “中心度” 分支抑制低质量检测的 bounding box。因此，FCOS能够输出与基于 anchor 的探测器差不多的召回率，如我们的实验所示。</p>
<h1 id="Our-Approach"><a href="#Our-Approach" class="headerlink" title="Our Approach"></a>Our Approach</h1><p>在本节中，我们首先以逐像素预测的方式重新构造目标检测任务。 接下来，我们展示了我们如何利用多级预测(multi-level prediction)来改善召回率并解决训练中重叠边界框导致的模糊性。 最后，我们展示了我们提出的 “center-ness” 分支，<strong>它有助于抑制低质量的检测边界框并大幅提高整体性能.</strong></p>
<h2 id="Fully-Convolutional-One-Stage-Object-Detector"><a href="#Fully-Convolutional-One-Stage-Object-Detector" class="headerlink" title="Fully Convolutional One-Stage Object Detector"></a>Fully Convolutional One-Stage Object Detector</h2><p>设 $F_i \in R^{H×W×C}$ 是 backbone CNN的第 $i$ 层的特征图，$s$ 是该层之前的 total stride。输入图像的 GT box 定义为 $\{B_i\}$，其中 $B_i = (x^{(i)}_0，y^{(i)}_0, x^{(i)}_1, y^{(i)}_1, c^{(i)} \in R^4 \times \{1, 2, …, C\}$. 其中 $(x^{(i)}_0, y^{(i)}_0)$ 和 $(x^{(i)}_1, y^{(i)}_1)$ 表示边界框的左上角和右下角的坐标。$c^(i)$ 是边界框中的对象所属的类。 C是类的数量，COCO数据集为80。</p>
<p>对于 feature map $F_i$ 上的每个位置 $(x,y)$，我们可以将其映射回输入图像的坐标 $(\frac{s}{2}+xs, \frac{s}{2}+ys)$，它差不多刚好位于位置 $(x, y)$ 的感受野中心附近。 与 anchor based detectors 将输入图像上的位置视为 anchor box 的中心并对这些 anchor box 的目标边界框进行回归不同，我们直接回归每个位置的目标边界框。 换句话说，<strong>我们的 Detector 直接将 location 视为训练样本而不是将 anchor box 视为训练样本</strong>，这与用于语义分割的FCN相同[16]。</p>
<p>具体而言，如果位置 $(x,y)$ 落入到任何 GT Box 内部, 那么久将其视为正样本, 并且该位置的类标签 $c^\star $ 就是 $B_i$ 的类标签。 否则它就是负样本并且 $c^\star = 0$（背景类）。 除了用于分类的标签之外，我们还有一个 4D 的实数向量 $t^\star = (l^\star，t^\star，r^\star，b^\star)$, 该向量是每个样本的回归目标。 这里 $l^\star，t^\star，r^\star，b^\star$ 是从 location 到 bbox 四条边的距离，如图1（左）所示。<strong>如果某个位置属于多个边界框，则会将其视为模糊样本</strong>。现在，<strong>我们只选择具有最小面积的边界框作为其回归目标(最简单的策略)</strong>。 在下一节中，我们将 <strong>展示通过多级预测，可以显著减少模糊样本的数量</strong>。 形式上，如果位置 $(x,y)$ 与边界框 $B_i$ 相关联，则该位置的训练回归目标可以表示为:</p>
<script type="math/tex; mode=display">\begin{cases}l^{\star} = x - x^{(i)}_0, t^\star = y - y^{(i)}_0 \\ r^\star = x^{(i)}_1 - x, b^\star = y^{(i)}_1 - y  \end{cases} \tag 1</script><p>值得注意的是，<strong>FCOS 可以利用尽可能多的前景样本来训练回归量。(GT box 内的每个像素点都是正样本)</strong> 它与基于 anchor 的探测器不同，anchor-based detectors 仅仅将与 GT box 具有足够 IOU 的anchor box 作为正样本。<strong>我们认为，这可能是 FCOS 优于 anchor-based 的原因之一。</strong></p>
<p><strong>Network Outputs:</strong><br>对应于 training targets，我们网络的最后一层会预测用于分类的 80D 向量 $\vec p$ 和 bounding box 坐标 4D 向量 $\vec t =(l，t，r，b)$。跟随 R-CNN 的做法，<strong>我们不是训练多类分类器，而是训练 $C$ 个二元分类器</strong>。与 R-CNN 类似，我们在 backbone 网络的特征图谱之后分别为分类和回归分支添加了 <strong>四个卷积层</strong>。此外，由于回归目标总是正的，我们使用 $exp(x)$ 将任意的实数都映射到回归分支顶部的 $(0, \infty)$。<strong>值得注意的是，FCOS 的网络输出变量比常用的 anchor based detectors 少 9 倍，其中每个位置有 9 个 anchor boxes</strong>.</p>
<p><strong>Loss Function:</strong></p>
<script type="math/tex; mode=display">L(\{\vec p_{x, y}\}, \{\vec t_{x, y}\}) = \frac{1}{N_{pos}} \sum_{x,y} L_{cls}(\vec p_{x, y}, c^{\star}_{x, y}) + \frac{\lambda}{N_{pos}} \sum_{x,y} I_{c^\star_{x,y} > 0} L_{reg} (\vec t_{x, y}, \vec t^\star_{x,y})  \tag 2</script><p>上式中, $L_{cls}$ 是 Focal Loss, $L_{reg}$ 是 IOU loss (正如 UnitBox 中的一样). N_{pos} 代表 positive samples 的数量, $\lambda$ 在本文中均为 1. 上面的求和是在 feature map $F_i I{c^\star_i &gt;0}$ 上所有的 locations 上进行的.</p>
<p><strong>Inference:</strong><br>FCOS 的 Inference 很简单。给定输入图像，我们将其放入网络进行一次 forward 计算, 并获得 feature map $F_i$ 上的每个位置的分类分数 $\vec p_{x,y}$ 和回归预测值 $\vec t_{x,y}$。 跟随 R-CNN 的设定，我们选择 $\vec p_{x,y} &gt; 0.05$ 的位置作为正样本并通过反转公式(1)来获得预测的边界框。</p>
<h2 id="Multi-level-Prediction-with-FPN-for-FCOS"><a href="#Multi-level-Prediction-with-FPN-for-FCOS" class="headerlink" title="Multi-level Prediction with FPN for FCOS"></a>Multi-level Prediction with FPN for FCOS</h2><p>在这里，我们展示了如何通过 FPN 的多级预测来解决所提出的 FCOS 存在的两个可能问题。</p>
<ol>
<li>CNN 中最后的 feature maps 的大步幅（例如，16）可能回导致相对较低的 best possible recall (BPR)。对于基于 anchor 的检测器，由于大步幅导致的低召回率可以通过降低 positive anchor boxes 所需的 IOU 分数来在一定程度上得到缓解。而对于 FCOS，乍一看可能认为其 BPR 会远低于基于 anchor 的检测器，因为 <strong>网络无法召回由于大步幅而最终在 feature map 上没有位置编码的对象</strong>。在这里，我们凭经验证明，即使步幅很大，基于 FCN 的 FCOS 仍然能够产生良好的BPR，它甚至可以比官方实现的 Detectron 中基于 anchor 的检测器 RetinaNet 的 BPR 更好。（参见表1）。因此，BPR 实际上不是 FCOS 无法解决的问题。此外，利用多级 FPN 预测，可以进一步改进 BPR 以匹配基于 anchor 的 RetinaNet 最佳BPR。</li>
<li>与 GT box 的多个重叠会导致在训练期间产生难以理解的模糊性，即哪个边界框应该在重叠位置进行回归？这种模糊性导致基于 FCN 的检测器的性能下降。在本文中，我们表明，<strong>使用多级预测可以极大地解决模糊性</strong>，并且与基于 anchor 的检测器相比，基于 FCN 的检测器可以获得相同的，有时甚至更好的性能。</li>
</ol>
<p>跟随 FPN 中的设定，我们在 <strong>不同级别的特征图上检测到不同大小的对象</strong>。 具体来说，我们使用定义为 $\{P_3，P_4，P_5，P_6，P_7\}$ 的五个级别的 feature map。$P_3，P_4$ 和 $P_5$ 由 backbone CNN 的特征图 $C_3，C_4$ 和 $C_5$ 和具有横向连接的1×1卷积层产生，如图2所示. $P_6$ 和 $P_7$ 通过分别在 $P_5$ 和 $P_6$ 上使用一个步长为 2 的卷积层产生. 最终，特征层级 $P_3，P_4，P_5，P_6$ 和 $P_7$ 具有的步幅分别为 8,16,32,64和128。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig2.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig2.jpg"></div></p>
<p>与 anchor based detectors 将不同大小的 anchor 分配给不同的特征级别不一样的是，<strong>我们直接限制边界框回归的范围</strong>。 更具体地说，我们首先计算所有特征级别上每个位置的回归目标 $l^\star，t^\star，r^\star$ 和 $b^\star$。 接下来，如果位置满足 $\max(l^\star，t^\star，r^\star，b^\star) &gt; m_i$ 或 $\max(l^\star，t^\star，r^\star，b^\star) &lt; m_{i-1}$ ，我们就将其设置为负样本并且再也不会对该位置进行回归操作。<strong>这里的 $m_i$ 是特征层级 $i$ 需要回归的最大距离</strong>。 在中文中，$m_2，m_3，m_4，m_5，m_6$ 和 $m_7$ 分别设置为 0,64,128,256,512 和 $\infty$。 由于 <strong>具有不同大小的对象被分配给不同的特征级别(这里是与 FoveaBox 的一处重要区别)</strong> 并且 <strong>大多数重叠发生在具有显著不同大小的对象之间</strong>，因此多级预测可以在很大程度上减轻上述模糊性并且将基于 FCN 的检测器提升到与基于 anchor 的检测器相同的检测性能，如我们后面的实验所示。</p>
<p>最后，跟随 R-CNN 和 Fast R-CNN 的设定，我们 <strong>共享不同特征级别之间的头部(这是与其他 Detector 的不同之处, 其他的都是每个特征层级独立的执行分类和回归)</strong>，这样不仅使检测器参数有效，而且能够提高检测性能。 然而，我们观察到不同的特征水平需要回归不同的尺寸范围（例如，$P_3$ 的尺寸范围是 [0,64] 而 $P_4$ 的尺寸范围是 [64,128]），因此 <strong>在不同的特征层使用相同的回归 heads 是不合理的。 故此, 我们不使用标准的 $exp(x)$，而是使用带有可训练标量 $s_i$ 的 $exp(s_i x)$ 来自动调整特征级 $P_i$ 的指数函数的基数，从而凭经验提高检测性能。</strong></p>
<h2 id="Center-ness-for-FCOS"><a href="#Center-ness-for-FCOS" class="headerlink" title="Center-ness for FCOS"></a>Center-ness for FCOS</h2><p>在 FCOS 中使用多级预测后，FCOS 和 anchor based 的检测器之间仍存在性能差距。 我们观察到这是由于 <strong>远离物体中心的位置产生的许多低质量预测边界框造成的。</strong></p>
<p>我们提出了一种简单而有效的策略来抑制这些低质量的检测边界框而不引入任何超参数。 具体来说，<strong>我们添加一个单层分支，与分类分支并行，以预测一个位置的 “中心概率(center-ness)”（即，从该位置到该位置所负责的对象的中心的距离）</strong> 如图2所示， 给定位置的回归目标 $l^\star，t^\star，r^\star$ 和 $b^\star$，center-ness target 定义为</p>
<script type="math/tex; mode=display">centerness^\star = \sqrt{\frac{\min(l^\star, r^\star)}{\max(l^\star, r^\star)} \times \frac{\min(t^\star, b^\star)}{\max(t^\star, b^\star)}} \tag 3</script><p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig3.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig3.jpg"></div></p>
<p>我们在这里使用 sqrt 来减缓中心的衰减。center-ness 从0到1，因此用 <strong>二元交叉熵（BCE）损失训练</strong>。 损失被添加到损失函数公式(2)中。 在测试时，通过 <strong>将预测的 center-ness 与相应的分类得分相乘来计算最终得分（用于对检测到的边界框进行排名）</strong>。 因此， <strong>center-ness 可以使远离物体中心的边界框的 scores 减小</strong>。 结果，这些低质量的边界框很可能被最终的非最大抑制（NMS）过程滤除，从而显著提高了检测性能。</p>
<p>基于 anchor 的检测器使用两个 IOU 阈值 $T_{low}$ 和 $T_{high}$ 将 anchor boxes 标记为负、忽略和正样本，center-ness 可以看作是一个 <strong>软阈值</strong>。它是在网络训练中学习的，不需要调整。此外，利用该策略，我们的检测器仍然可以将任何落在 GT Box 中的位置视为正样本，除了上述多层预测中设置为负样本的位置外，这样就可以为回归器使用尽可能多的训练样本。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><strong>Training Details:</strong> 略…</p>
<p><strong>Inference Details:</strong> 我们首先通过网络对输入图像执行 forward，并获得具有预测类的预测边界框。 其他的后处理与 RetinaNet 完全相同，我们直接使用 RetinaNet 相同的后处理超参数（例如NMS的阈值）。 我们认为，如果超参数进行了针对性的优化，探测器的性能还可以进一步提高。 我们使用与训练相同大小的输入图像(shorter 800, longer less 1333)。</p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><h3 id="Multi-level-Prediction-with-FPN"><a href="#Multi-level-Prediction-with-FPN" class="headerlink" title="Multi-level Prediction with FPN"></a>Multi-level Prediction with FPN</h3><p>如前所述，基于 FCN 的探测器的主要问题是 <strong>低召回率</strong> 和 <strong>与 GT Box 重叠导致的模糊样本</strong>。 在本节中，我们展示了可以通过多级预测在很大程度上解决这两个问题。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab1.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab1.jpg"></div></p>
<p><strong>Bset Possible Recalls:</strong> 基于fcn的检测器的第一个问题是，它可能不能提供良好的最佳可能召回(BPR)。在本节中，我们表示这种关注是不必要的。在这里，BPR被定义为检测器最多能召回的 GT 与 GT 数量之比。如果将ground-truth框分配给至少一个样本(即， FCOS中的一个位置或 anchor based 的检测器中的锚盒)。如表1所示，只有feature level P4, stride为16(即，没有FPN)， FCOS已经可以获得95.55%的BPR。在官方实现检测器中，BPR远远高于基于 anchor 检测器的90.92%的BPR，在官方实现检测器中，只有IOU≥0.4的低质量匹配被使用。在FPN的帮助下，FCOS可以达到98.40%的BPR，非常接近 anchor based 点的检测器使用所有低质量匹配所能达到的最佳BPR。由于这些检测器的精度-召回曲线(如补充材料所示)的最佳召回率远低于90%，因此FCOS与基于 anchor 的检测器之间较小的BPR间隙实际上并不会影响检测器的性能。表4也证实了这一点，其中FCOS实现的AR甚至比基于anchor的同类产品更好。因此，对低BPR的关注可能没有必要。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab4.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab4.jpg"></div></p>
<p><strong>Ambiguous Samples:</strong> 关于基于FCN的探测器的另一个问题是，由于地面实况边界框中的重叠，它可能具有大量模糊样本，如图1（右）所示。在表2中，我们显示了模糊样本与所有正样本在 minival split 上的比率。如表中所示，如果不使用FPN并且仅使用特征级别P4，则确实存在大量模糊样本（23.16％）。但是，如果我们使用所有特征级别，则比率可以显着降低到仅7.14％，因为大多数重叠对象都分配给不同的特征级别。此外，我们认为，由于边界框预测这些样本总是可以与正确的类别匹配而不管w.r.t，因此相同类别的对象之间的重叠导致的模糊样本在推断时无关紧要。哪个对象样本回归。因此，我们 <strong>只计算具有不同类别的边界框之间重叠的模糊样本</strong>。如表2所示，多级预测将模糊样本的比率从17.84％降低到3.75％。为了进一步表明地面实况框中的重叠不是我们基于FCN的FCOS的问题，我们计算在推断有多少检测到的边界框来自模糊位置时。我们发现只有2.3％的检测到的边界框是由不明确的位置产生的。通过进一步仅考虑不同类别之间的重叠，该比率降低至1.5％。如以下实验所示，极低的重叠率不会使我们的FCOS低于 anchor based 的探测器。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab2.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab2.jpg"></div></p>
<p><strong>Detection Performance:</strong> 到目前为止，我们已经证明了FCOS的BPR已经足够，多级预测不仅可以改善BPR，而且还可以显着减少训练期间的边界框回归的模糊性。 如表3所示，在多级预测的帮助下，基于FCN的FCOS已经可以实现与具有多级预测的 anchor based 的RetinaNet相同的级别性能（33.8％对35.7％）。 与仅具有一个特征级别P4的那个相比，AP几乎翻倍。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab3.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab3.jpg"></div></p>
<h3 id="With-or-Without-Center-ness"><a href="#With-or-Without-Center-ness" class="headerlink" title="With or Without Center-ness"></a>With or Without Center-ness</h3><p>我们已经证明基于FCN的FCOS能够与 anchor based 的检测器RetinaNet实现相当的性能。但是，AP的性能差距仍然在2％左右。我们认为这种差距可能是由于远离物体中心的位置产生了一些低质量的检测边界框。很容易看出，靠近中心的位置更有可能产生更准确的预测。因此，应该为远距离位置产生的检测分配低置信度分数。为此，我们利用中心分支来抑制低质量检测到的边界框。如表5所示，<strong>中心分支可将AP从33.8％提升至36.6％，优于 anchor based 的探测器的性能（35.7％）</strong>。可以注意到，也可以使用预测的回归向量计算中心，而不引入额外的中心分支。然而，如表5所示，从回归向量计算的中心不能改善性能，因此需要单独学习的中心。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab5.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab5.jpg"></div></p>
<p>为了进一步证明中心的有用性，我们再进行一次实验。 我们假设我们有一个在推理期间提供 GT 中心得分的神谕。 通过保持所有其他设置完全相同，用于推理的 GT 中心显着地将AP提高到42.1，这意味着有足够的空间进一步提高我们目前36.6 AP的准确度，如表5所示，只要我们提高中心的预测准确性。<br>因此，我们使中心分支的层数更深，具有与分类和回归分支相同的体系结构，从而将AP从36.6提高到36.8。<br>理论上，我们甚至可以训练一个单独的深度网络，它与主探测器不共享任何权重，其唯一目的是预测中心分数。 这是唯一可能的，因为中心分数仅用于推理。 因此，我们能够将中心预测器的训练与探测器的训练分离。 这种解耦使我们能够以额外的计算复杂度来设计最好的中心预测器。 我们还假设所有其他检测器，如果需要NMS进行后处理，可能能够从这种准确的中心得分预测器中受益。 我们将此主题留待将来工作。</p>
<h3 id="FCOS-vs-Anchor-based-Detectors"><a href="#FCOS-vs-Anchor-based-Detectors" class="headerlink" title="FCOS vs. Anchor-based Detectors"></a>FCOS vs. Anchor-based Detectors</h3><p>上述FCOS与标准RetinaNet有两个细微差别。 1）我们在新添加的卷积层中使用组标准化（Group Normalization）[23]，除了最后的预测层，这使我们的训练更稳定。 2）<strong>我们使用P5在标准RetinaNet中生产P6和P7而不是C5</strong>。 我们观察到使用P5可以略微提高性能。</p>
<p>为了证明我们的FCOS可以作为基于 anchor 的探测器的简单而强大的替代方案，并且为了公平比较，我们将GN添加到RetinaNet中并在我们的探测器中使用C5。 如表4所示，在完全相同的设置下，我们的FCOS仍然优于 anchor based 的探测器。 由于我们的基于FCN的探测器具有许多优点（例如，设计复杂性更少，并且仅使用表4中所示的训练内存占用的一半）而不是基于 anchor 的探测器，因此我们鼓励社区重新考虑锚的必要性 对象检测中的框。 此外，值得注意的是，我们直接使用来自RetinaNet的所有超参数（例如，学习速率，NMS阈值等），其已经针对 anchor based 的检测器进行了优化。 我们认为，如果对超参数进行调整，FCOS的性能可以进一步提高。</p>
<p>大家可能仍然担心边界框中的重叠会导致性能下降。 为了进一步表明重叠不是FCOS的问题，我们构造了一个迷你的子集，称为 minival_overlapped。 它由3986个图像组成，每个图像包括至少一个重叠的边界框。 该子集总共包含35,058个边界框，其中30,625个边界框（最多87％）与其他边界框重叠。 在子集上，我们的FCOS仍然比 anchor based 的RetinaNet获得更好的性能，如表6所示. 这表明FCOS可以很好地处理重叠的边界框。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab6.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab6.jpg"></div></p>
<h2 id="Comparison-with-State-of-the-art-Detectors"><a href="#Comparison-with-State-of-the-art-Detectors" class="headerlink" title="Comparison with State-of-the-art Detectors"></a>Comparison with State-of-the-art Detectors</h2><p>在消融研究中，为了与 anchor based 的对应物进行公平比较，并证明我们的框架可以作为 anchor based 的检测器的强大而简单的替代方案，我们直接利用RetinaNet的所有超参数。我们认为，如果为我们的探测器调整超参数，性能可以大大提高。对于我们关于test-dev拆分的主要结果，我们在训练期间使用RetinaNet中的 scale jitter，并将迭代次数加倍。其他设置与消融研究完全相同。如表7所示，以ResNet-101-FPN和ResNet-32x8d-101-FPN为骨干，我们的FCOS在相同的骨干网上的性能分别比同样的骨干网分别高出1.9％和1.3％。据我们所知，这是第一次没有任何花里胡哨tricks的 anchor free 探测器在很大程度上优于 anchor based 的探测器。 FCOS也大大优于其他经典的两级 anchor based 点的探测器，例如更快的R-CNN。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab7.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab7.jpg"></div></p>
<p>与最近最先进的单级探测器CornerNet [10]相比，我们的FCOS在AP中也有0.5％的增益。也许增益相对较小，但我们的探测器比CornerNet具有以下优势。 1）我们使用更快更简单的主干ResNet-101而不是CornerNet中的Hourglass-104来实现性能。 2）除检测任务中的标准后处理NMS外，我们的检测器不需要任何其他后处理。相比之下，CornerNet需要将具有嵌入向量的角对分组，这需要对检测器进行特殊设计。 3）与CornerNet相比，我们认为我们的FCOS更有可能成为当前主流 anchor based 点的探测器的强大而简单的替代方案。</p>
<h1 id="Extensions-on-Region-Proposal-Networks"><a href="#Extensions-on-Region-Proposal-Networks" class="headerlink" title="Extensions on Region Proposal Networks"></a>Extensions on Region Proposal Networks</h1><p>到目前为止，我们已经证明，在单级探测器中，我们的FCOS可以实现比 anchor based 的对应物更好的性能。 直观地说，FCOS也应该能够在两级探测器更快的R-CNN中用FPN [11]替换区域提议网络（RPN）中的锚箱。 在本节中，我们通过实验确认。<br>与具有FPN [11]的RPN相比，我们用FCOS中的方法替换锚箱。 此外，我们将GN添加到FPN头中的层中，这可以使我们的训练更加稳定。 所有其他设置与官方代码[6]中带有FPN的RPN完全相同。 如表8所示，即使没有提出的中心分支，我们的FCOS也已经显着改善了AR100和AR1k。 通过拟议的中心分支，FCOS进一步将AR100和AR1k分别提升至52.8％和60.3％，AR100的相对改善为21％，AR1k的绝对改善比FPN的官方RPN高3％。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab8.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab8.jpg"></div></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种 anchor freee, proposal free 的单级探测器FCOS。 如实验所示，FCOS与流行的 anchor based 的一级探测器相比，包括RetinaNet，YOLO和SSD，但设计复杂性要低得多。 FCOS完全避免了与锚框相关的所有计算和超参数，并以每像素预测方式解决了对象检测，类似于其他密集预测任务，例如语义分割。 FCOS还在一级探测器中实现了最先进的性能。 我们还表明，FCOS可用作两级探测器中的RPN，速度更快的R-CNN，并且大幅优于其RPN。 鉴于其有效性和效率，我们希望FCOS可以作为当前主流锚点探测器的强大而简单的替代方案。 我们还相信FCOS可以扩展到解决许多其他实例级识别任务。</p>
<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="Class-agnostic-Precision-recall-Curves"><a href="#Class-agnostic-Precision-recall-Curves" class="headerlink" title="Class-agnostic Precision-recall Curves"></a>Class-agnostic Precision-recall Curves</h2><p>在图4，图5和图6中，我们给出了在IOU阈值分别为0.50,0.75和0.90的 minival split 的类别不可知的 PR 曲线。 表9显示了对应于三条曲线的AP。<br>如表9所示，我们的FCOS比其基于锚的对应物RetinaNet具有更好的性能。 此外，值得注意的是，通过更严格的IOU阈值，FCOS比RetinaNet有更大的改进，这表明FCOS有更好的边界框回归器来更准确地检测对象。 其中一个原因应该是FCOS能够利用更多前景样本来训练回归量，如我们的主要论文中所述。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig4.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig4.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig5.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig5.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig6.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig6.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/tab9.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ftab9.jpg"></div></p>
<p>最后，如所有精确回忆曲线所示，这些探测器在精确回忆曲线中的最佳回忆率远低于90％。 它进一步表明，FCOS和RetinaNet之间的最佳可能召回（BPR）的小差距（98.40％对比99.23％）几乎不会损害最终的检测性能。</p>
<h2 id="Visualization-for-Center-ness"><a href="#Visualization-for-Center-ness" class="headerlink" title="Visualization for Center-ness"></a>Visualization for Center-ness</h2><p>正如我们的主要论文中所提到的，通过抑制低质量检测到的边界框，所提出的中心分支可以大幅提高检测性能。在本节中，我们确认了这一点。<br>我们期望中心可以降低低质量边界框的分数，使得可以在诸如非最大抑制（NMS）的后续处理中过滤掉这些边界框。如果检测到的边界框与其对应的地面实况边界框具有较低的IOU分数，则该边界框被认为是低质量的边界框。具有低IOU但高置信度得分的边界框可能会成为误报并损害精度。<br>在图7中，我们将检测到的边界框视为2D点（x，y），其中x是其得分，y是具有其对应的地面实况框的IOU。如图7（左）所示，在应用中心之前，存在大量低质量边界框但具有高置信度分数（即，线y = x下的点）。由于它们的高分，在后处理中不能消除这些低质量的边界框并导致降低检测器的精度。在将分类得分与中心得分相乘之后，将这些点推到图的左侧（即，它们的得分减小），如图7（右）所示。结果，这些低质量的边界框更可能在后处理中被滤除，并且可以提高最终的检测性能。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig7.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig7.jpg"></div></p>
<h1 id="论文亮点"><a href="#论文亮点" class="headerlink" title="论文亮点"></a>论文亮点</h1><p>TL;DR<br>本文提出了一种新的基于全卷积的 anchor free 目标检测方法, 抛弃了传统的 anchor 设定, 以每个像素点距离四个边框的距离作为回归目标, 通过多级特征和center-ness的加持, 取得了超过anhor-based方法的性能表现</p>
<p>Algorithm<br>首先, 作者重新定义了 bounding box 的回归方式, 以此抛弃 anchor box 的设定, 新的回归方式如下图, 对于每一个像素点, 其回归目标是该点到四个边界的距离, 当一个像素点落在 gt 物体内部时, 认为他是正样本, 反之, 认为是负样本</p>
<p>由于同一个像素点可能会属于多个不同的物体, 因此, 为了解决这个问题, 作者提出用特征金字塔的不同层来控制不同物体的预测, 由于交并比高的物体, 往往具有不同的尺寸, 因此, 将它们分别放在不同的特征图谱上去预测, 可以有效的解决这个模糊样本带来的问题.</p>
<p>由于每个像素点都可以参与预测, 这样就会造成很多低质量预测结果的产生, 对于一个物体来说, 远离物体中心点的的像素点, 往往更容易生成低质量的框, 因此, 作者提出对于靠近中心的像素点, 我们赋予它更大的权重, 远离的则赋予更小的权重, 这样, 在后续的nms过程中, 就可以消除大部分的低质量框</p>
<p>anchor box 固然可以帮助神经网络更好的预测物体框, 但是 anchor box 会带来大量的超参数和计算量, 因此, 这种 anchor free 的想法非常值得尝试, 有望在以后取代 anchor based 的目标检测方法.</p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><h1 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h1><h1 id="Related-Work-1"><a href="#Related-Work-1" class="headerlink" title="Related Work"></a>Related Work</h1><h1 id="Our-Approach-1"><a href="#Our-Approach-1" class="headerlink" title="Our Approach"></a>Our Approach</h1>
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/目标检测/" rel="tag"><i class="fa fa-tag"></i> 目标检测</a>
          
            <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a>
          
            <a href="/tags/论文解读/" rel="tag"><i class="fa fa-tag"></i> 论文解读</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/z_post/计算机视觉-FSAF-CVPR2019/" rel="prev" title="FSAF (CVPR, 2019)">
                <i class="fa fa-chevron-left"></i> FSAF (CVPR, 2019)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/z_post/计算机视觉-ExtremeNet-CVPR2019/" rel="next" title="ExtremeNet (CVPR, 2019)">
                ExtremeNet (CVPR, 2019) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar_zz.png"
                alt="ZeroZone" />
            
              <p class="site-author-name" itemprop="name">ZeroZone</p>
              <p class="site-description motion-element" itemprop="description">吾乃闪耀的芝士蛋挞!</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">268</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">41</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/hellozhaozheng" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:hellozhaozheng@foxmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/ksws0292756" title="零域CSDN博客" target="_blank">零域CSDN博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://xinghanzzy.github.io/" title="BoXiao的博客" target="_blank">BoXiao的博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://oldpan.me/" title="Oldpan的博客" target="_blank">Oldpan的博客</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Abstract"><span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Related-Work"><span class="nav-text">Related Work</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Our-Approach"><span class="nav-text">Our Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Fully-Convolutional-One-Stage-Object-Detector"><span class="nav-text">Fully Convolutional One-Stage Object Detector</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-level-Prediction-with-FPN-for-FCOS"><span class="nav-text">Multi-level Prediction with FPN for FCOS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Center-ness-for-FCOS"><span class="nav-text">Center-ness for FCOS</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Experiments"><span class="nav-text">Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ablation-Study"><span class="nav-text">Ablation Study</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-level-Prediction-with-FPN"><span class="nav-text">Multi-level Prediction with FPN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#With-or-Without-Center-ness"><span class="nav-text">With or Without Center-ness</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCOS-vs-Anchor-based-Detectors"><span class="nav-text">FCOS vs. Anchor-based Detectors</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comparison-with-State-of-the-art-Detectors"><span class="nav-text">Comparison with State-of-the-art Detectors</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Extensions-on-Region-Proposal-Networks"><span class="nav-text">Extensions on Region Proposal Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Appendix"><span class="nav-text">Appendix</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Class-agnostic-Precision-recall-Curves"><span class="nav-text">Class-agnostic Precision-recall Curves</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visualization-for-Center-ness"><span class="nav-text">Visualization for Center-ness</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#论文亮点"><span class="nav-text">论文亮点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-1"><span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Related-Work-1"><span class="nav-text">Related Work</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Our-Approach-1"><span class="nav-text">Our Approach</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZeroZone</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">2.7m</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
    <span title="站点阅读时长">41:31</span>
  
</div>










  <div class="footer-custom">勤练带来力量</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  





  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: true,
        appId: 'o5ny24Rtrv0pjlRYjBoj9rfz-gzGzoHsz',
        appKey: 'o9SAGYkO04n5xjXkeWXaq1pm',
        placeholder: '无需注册即可评论, 支持在 Gravatar(https://cn.gravatar.com) 上自定义头像, 评论时只需填写对应邮箱即可显示自定义头像, 邮箱不会暴露在评论处, 大可放心, 由于无登陆选项, 因此邮箱会作为我联系你的唯一方式',
        avatar:'',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
