<!DOCTYPE html>













<html class="theme-next gemini" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">

<meta name="google-site-verification" content="jgw73iXouBAJcOuff0yi9vdSNDecBSOUXacsHJszpmo" />
<meta name="baidu-site-verification" content="xyf9WD2vvl" />











<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/apple-icon-57x57.png?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_body":"slideDownIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛；如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。 使用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。 使用交叉熵损失函数，误差越大参数的梯度也越大，能够快速收敛。 总目录篇机器学习: 逻辑回归, 支持向量机, 决策树, 降维">
<meta name="keywords" content="知识点梳理,计算机视觉">
<meta property="og:type" content="article">
<meta property="og:title" content="【置顶】计算机视觉知识点总结">
<meta property="og:url" content="https://hellozhaozheng.github.io/z_post/面试-计算机视觉知识点总结/index.html">
<meta property="og:site_name" content="从零开始的BLOG">
<meta property="og:description" content="神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛；如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。 使用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。 使用交叉熵损失函数，误差越大参数的梯度也越大，能够快速收敛。 总目录篇机器学习: 逻辑回归, 支持向量机, 决策树, 降维">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/d7b90c85ly1fww4hqied3j21kw0esjwt.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/d7b90c85ly1g0ixnfk7w4j20o408q3yr.jpg">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/d7b90c85ly1fxmo2fpvavj21650u079w.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/d7b90c85ly1g1g5nupi5fj21c80u0doq.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/d7b90c85ly1g1g6on0w08j21hc0u0b29.jpg">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/d7b90c85ly1g1g6orzjmmj21hc0u0b29.jpg">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/d7b90c85ly1g1g6ox6512j21hc0u0e81.jpg">
<meta property="og:image" content="https://wx3.sinaimg.cn/large/d7b90c85ly1fxc7ewvfr1j20u00wn0y7.jpg">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/d7b90c85ly1fxc7gghlzjj21xt0l1n26.jpg">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/d7b90c85ly1fxsf9g1udsj21q00s3152.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/d7b90c85ly1fx1toyw0xkj20kc0a5wkw.jpg">
<meta property="og:image" content="https://wx3.sinaimg.cn/large/d7b90c85ly1g1hbfdtyvcj21dg0m6ta3.jpg">
<meta property="og:image" content="https://hellozhaozheng.github.io/z_post/面试-计算机视觉知识点总结/Batch-Normalization深入解析">
<meta property="og:updated_time" content="2019-07-07T11:28:12.596Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【置顶】计算机视觉知识点总结">
<meta name="twitter:description" content="神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛；如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。 使用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。 使用交叉熵损失函数，误差越大参数的梯度也越大，能够快速收敛。 总目录篇机器学习: 逻辑回归, 支持向量机, 决策树, 降维">
<meta name="twitter:image" content="https://wx2.sinaimg.cn/large/d7b90c85ly1fww4hqied3j21kw0esjwt.jpg">






  <link rel="canonical" href="https://hellozhaozheng.github.io/z_post/面试-计算机视觉知识点总结/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>【置顶】计算机视觉知识点总结 | 从零开始的BLOG</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?21a4899cc63d3c11a3d90ac58074a19c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">从零开始的BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">与其感慨路难行，不如马上出发</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档<span class="badge">262</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-计算机视觉">
    <a href="/categories/计算机视觉/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tripadvisor"></i> <br />计算机视觉</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-深度学习">
    <a href="/categories/深度学习/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-drupal"></i> <br />深度学习</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-caffe2">
    <a href="/categories/Caffe2/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-coffee"></i> <br />Caffe2</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-pytorch">
    <a href="/categories/PyTorch/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-free-code-camp"></i> <br />PyTorch</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-c++">
    <a href="/categories/Cpp/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-codiepie"></i> <br />C++</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-python">
    <a href="/categories/Python/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-product-hunt"></i> <br />Python</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-项目">
    <a href="/categories/项目/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-connectdevelop"></i> <br />项目</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-cuda">
    <a href="/categories/CUDA/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-braille"></i> <br />CUDA</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-其他">
    <a href="/categories/其他/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />其他</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签<span class="badge">40</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于我</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />站内搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="站内搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  
  
    
      
    
    <a href="https://github.com/hellozhaozheng" class="github-corner" target="_blank" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#222; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg>
    
      </a>
    



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://hellozhaozheng.github.io/z_post/面试-计算机视觉知识点总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZeroZone">
      <meta itemprop="description" content="并不是什么厉害的地方<br>只是一个安静的学习角落">
      <meta itemprop="image" content="/images/avatar_zz.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="从零开始的BLOG">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">【置顶】计算机视觉知识点总结
              
            
          </h1>
        

        <div class="post-meta">
	
	     <i class="fa fa-thumb-tack"></i>
	    <font style="color:#999">置顶</font>
	    <span class="post-meta-divider">|</span>
	
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-23 20:32:39" itemprop="dateCreated datePublished" datetime="2018-10-23T20:32:39+08:00">2018-10-23</time>
            

            
          </span>

	  
  	    <span class="post-updated">
    		&nbsp; | &nbsp; 更新于
    		<time itemprop="dateUpdated" datetime="2019-07-07T19:28:12+08:00" content="2019-07-07">
      		  2019-07-07
    		</time>
  	  </span>
	  

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/面试/" itemprop="url" rel="index"><span itemprop="name">面试</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">9.5k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">9 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛；如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。</p>
<p>使用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。</p>
<p>使用交叉熵损失函数，误差越大参数的梯度也越大，能够快速收敛。</p>
<h1 id="总目录篇"><a href="#总目录篇" class="headerlink" title="总目录篇"></a>总目录篇</h1><p><strong>机器学习:</strong> <a href="#逻辑回归">逻辑回归</a>, <a href="#支持向量机">支持向量机</a>, <a href="#决策树">决策树</a>, <a href="#降维">降维</a>, <a href="#聚类">聚类</a></p>
<p><strong>深度学习:</strong> <a href="#优化方法">优化方法</a>, <a href="#初始化方法">初始化方法</a>, <a href="#损失函数">损失函数</a>, <a href="#激活函数">激活函数</a>, <a href="#正则化">正则化</a>, <a href="#归一化">归一化</a>, <a href="#感受野">感受野</a>, <a href="#全连接层">全连接层</a>, <a href="#卷积层">卷积层</a>, <a href="#反卷积层">反卷积层</a>, <a href="#池化层">池化层</a>, <a href="#训练问题">训练问题</a></p>
<p><strong>网络结构:</strong> <a href="#AlexNet">AlexNet</a>, <a href="#VGGNet">VGGNet</a>, <a href="#InceptionV1">InceptionV1</a>, <a href="#InceptionV2">InceptionV2</a>, <a href="#InceptionV3">InceptionV3</a>, <a href="#InceptionV4">InceptionV4</a>, <a href="#Xception">Xception</a>, <a href="#ResNet">ResNet</a>, <a href="#ResNeXt">ResNeXt</a>, <a href="#DenseNet">DenseNet</a>, <a href="#SqueezeNet">SqueezeNet</a>, <a href="#MobileNet">MobileNet</a>, <a href="#MobileNetV2">MobileNetV2</a>, <a href="#ShuffleNet">ShuffleNet</a>, <a href="#ShuffleNetV2">ShuffleNetV2</a>, <a href="#SENetV2">SENet</a>,</p>
<p><strong>目标检测:</strong> <a href="#NMS">NMS</a></p>
<p><strong>图像处理:</strong> <a href="#图像放缩">图像放缩</a></p>
<p><strong>数学基础:</strong> <a href="#概率分布">概率分布</a>, 矩阵乘法优化</p>
<p><strong>常见问题:</strong><br><a href="../面试-计算机视觉问题汇总">面试-计算机视觉问题汇总</a></p>
<ul>
<li><a href="../面试-计算机视觉问题汇总/#简述一下现在的 SOTA 目标检测模型">简述一下现在的 SOTA 目标检测模型</a></li>
<li><a href="../面试-计算机视觉问题汇总/#简要说一下 SSD, FPN, RefineDet, PFPNet, STDN, M2Det 等特征金字塔的区别">简要说一下 SSD, FPN, RefineDet, PFPNet, STDN, M2Det 等特征金字塔的区别</a></li>
<li><a href="../面试-计算机视觉问题汇总/#你知道有哪些常用的训练 Trick">你知道有哪些常用的训练 Trick</a></li>
<li><a href="../面试-计算机视觉问题汇总/#有哪些数据增广方法? 怎么实现的?">有哪些数据增广方法? 怎么实现的?</a></li>
<li><a href="../面试-计算机视觉问题汇总/#简单说一下 PyTorch 和 TensorFlow 的区别">简单说一下 PyTorch 和 TensorFlow 的区别</a></li>
<li><a href="../面试-计算机视觉问题汇总/#你觉得目标检测领域还有哪些可以继续改进或者优化的地方">你觉得目标检测领域还有哪些可以继续改进或者优化的地方</a></li>
</ul>
<h1 id="机器学习篇"><a href="#机器学习篇" class="headerlink" title="机器学习篇"></a>机器学习篇</h1><p>各种机器学习算法的应用场景分别是什么(比如朴素贝叶斯、决策树、K 近邻、SVM、逻辑回归最大熵模型)<br><a href="https://www.zhihu.com/question/26726794/answer/151282052" target="_blank" rel="noopener">https://www.zhihu.com/question/26726794/answer/151282052</a></p>
<p><span id="基本概念"></span></p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li>PR 曲线</li>
<li>ROC 曲线</li>
</ul>
<p><span id="逻辑回归"></span></p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p><a href="../机器学习-逻辑回归">逻辑回归与线性回归</a></p>
<ul>
<li><a href="../机器学习-逻辑回归/#逻辑回归和线性回归的定义">逻辑回归和线性回归的定义</a></li>
<li><a href="../机器学习-逻辑回归/#逻辑回归与线性回归的联系和区别">逻辑回归和线性回归的联系和区别</a></li>
<li><a href="../机器学习-逻辑回归/#对于一个二分类问题">对于一个二分类问题, 如果数据集中存在一些离异值, 在不清洗数据的情况下, 选择逻辑回归还是 SVM? 为什么?</a></li>
<li><a href="../机器学习-逻辑回归/#逻辑回归和 SVM 的区别是什么">逻辑回归与 SVM 的区别是什么? 哪个是参数模型? 分别适合在什么情况下使用?</a></li>
</ul>
<p><span id="支持向量机"></span></p>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><p><a href="../机器学习-SVM深入解析">SVM深入解析</a></p>
<ul>
<li><a href="../机器学习-SVM深入解析/#简述 SVM 的基本概念和原理">简述 SVM 的基本概念和原理</a></li>
<li><a href="../机器学习-SVM深入解析/#SVM 推导过程">SVM 推导过程</a></li>
<li><a href="../机器学习-SVM深入解析/#SVM 如何解决线性不可分问题">SVM 如何解决线性不可分问题</a></li>
<li><a href="../机器学习-SVM深入解析/#为什么SVM的分类结果仅依赖于支持向量?">为什么SVM的分类结果仅依赖于支持向量?</a></li>
<li><a href="../机器学习-SVM深入解析/#如何选取核函数">如何选取核函数</a></li>
<li><a href="../机器学习-SVM深入解析/#为什么说高斯核函数将原始特征空间映射成了无限维空间?">为什么说高斯核函数将原始特征空间映射成了无限维空间?</a></li>
<li><a href="../机器学习-SVM深入解析/#核函数中不同参数的影响">核函数中不同参数的影响</a></li>
<li><a href="../机器学习-SVM深入解析/#既然深度学习技术性能表现已经全面超越 SVM, SVM 还有存在的必要吗?">既然深度学习技术性能表现以及全面超越 SVM, SVM 还有存在的必要吗?</a></li>
</ul>
<p><span id="决策树"></span></p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p><span id="降维"></span></p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p><span id="聚类"></span></p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><ul>
<li><a href="../机器学习-聚类分析/#简单介绍常用的聚类方法">简单介绍常用的聚类方法</a></li>
</ul>
<h1 id="深度学习篇"><a href="#深度学习篇" class="headerlink" title="深度学习篇"></a>深度学习篇</h1><p><span id="优化方法"></span></p>
<h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p><strong>梯度下降:</strong> SGD, Momentum, Nesterov, Adagrad, Adadelta, RMSprop, Adam, Adamax<br><strong>牛顿法:</strong><br><strong>拟牛顿法:</strong><br><strong>共轭梯度法:</strong></p>
<ul>
<li><a href="../深度学习-各种优化方法整理总结/#简述各种优化方法的概念及其优缺点">简述各种优化方法的概念及其优缺点</a></li>
<li><a href="../深度学习-各种优化方法整理总结/#简述 Adam 中使用的指数加权滑动平均法">简述 Adam 中使用的指数加权滑动平均法</a></li>
<li><p><a href="../深度学习-各种优化方法整理总结/#各种优化方法的源码实现">各种优化方法的源码实现</a></p>
</li>
<li><p>各个优化算法的形式, 优点和缺点</p>
</li>
<li>Adam 无法收敛?: <a href="https://www.jiqizhixin.com/articles/2017-12-06" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-12-06</a></li>
<li>SGD 的参数设置</li>
</ul>
<p><a href="https://www.cnblogs.com/happylion/p/4172632.html" target="_blank" rel="noopener">https://www.cnblogs.com/happylion/p/4172632.html</a></p>
<p><a href="https://www.cnblogs.com/shixiangwan/p/7532830.html" target="_blank" rel="noopener">https://www.cnblogs.com/shixiangwan/p/7532830.html</a></p>
<p><a href="https://www.cnblogs.com/hlongch/p/5734105.html" target="_blank" rel="noopener">https://www.cnblogs.com/hlongch/p/5734105.html</a></p>
<p><a href="https://www.baidu.com/link?url=8EyCqGYnldJzHuqBBGagV9juEA_nhCYvRElM2Tw0lBdewSmc0qshAy_AHAEegO-wT3vLsrcY1xSDdyLOmL09Ltm_UICAFX_C02QdkkSCcWW&amp;wd=&amp;eqid=ce9adcb10004685c000000035b5d4fb6" target="_blank" rel="noopener">https://www.baidu.com/link?url=8EyCqGYnldJzHuqBBGagV9juEA_nhCYvRElM2Tw0lBdewSmc0qshAy_AHAEegO-wT3vLsrcY1xSDdyLOmL09Ltm_UICAFX_C02QdkkSCcWW&amp;wd=&amp;eqid=ce9adcb10004685c000000035b5d4fb6</a></p>
<p><a href="../深度学习-各种优化方法整理总结">各种优化方法整理总结</a></p>
<p><a href="https://mp.weixin.qq.com/s/lh4jTYJroq6AKb2fYsP-GQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/lh4jTYJroq6AKb2fYsP-GQ</a></p>
<p><span id="初始化方法"></span></p>
<h2 id="初始化方法"><a href="#初始化方法" class="headerlink" title="初始化方法"></a>初始化方法</h2><p>constant, uniform, gaussian, xavier, msra(kaiming), bilinear</p>
<ul>
<li>各个初始化方法的形式,</li>
<li>神经网络训练时是否可以将全部参数初始化为 0?</li>
</ul>
<p><a href="../深度学习-各种初始化方法深入分析">深度学习-各种初始化方法深入分析</a></p>
<p><span id="损失函数"></span></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>绝对值损失(L1), 平方损失(L2), Binary 交叉熵, Softmax 交叉熵</p>
<p><a href="../深度学习-各种损失函数深入解析">深度学习-各种损失函数深入解析</a></p>
<ul>
<li>写出多层感知机的平方误差和交叉熵误差损失函数</li>
<li>推导平方误差和交叉熵误差损失函数的各层参数更新的梯度计算公式</li>
</ul>
<p><span id="激活函数"></span></p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>Sigmoid, Tanh, ReLU, Leaky ReLU, PReLU, RReLU, ELU, Maxout</p>
<ul>
<li>写出常用的激活函数的公式及其导数形式</li>
<li>简单画出常用激活函数的图像</li>
<li>为什么需要激活函数?</li>
<li>各个激活函数的优缺点和适用场景</li>
<li>Sigmoid 激活函数和 Softmax 激活函数的区别</li>
<li>什么情况下 ReLU 的神经元会死亡? 为什么? 如何解决?</li>
<li>激活函数的使用原则</li>
</ul>
<p><a href="../深度学习-各种激活函数深入解析">深度学习-各种激活函数深入解析</a></p>
<p><span id="正则化"></span></p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>L1, L2</p>
<p><a href="../深度学习-正则化方法深入解析">深度学习-正则化方法深入解析</a></p>
<ul>
<li>L1 正则和 L2 正则的特点是什么? 各有什么优势?</li>
<li>L1 和 L2 的区别有哪些?</li>
<li>L1正则化使模型参数稀疏的原理是什么?</li>
<li>为什么 L1 和 L2 分别对应拉普拉斯先验和高斯先验?</li>
<li>为什么权重矩阵稀疏可以防止过拟合?</li>
<li>为何权重参数 $w$ 减小就可以防止过拟合?</li>
<li>L0 范式和 L1 范式都能实现稀疏, 为什么不选择用 L0 而要用 L1?</li>
<li>为什么说 L2 范式可以优化计算?</li>
<li>正则项前面的系数一般怎么设置?</li>
</ul>
<p><span id="归一化"></span></p>
<h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><p><div style="width: 550px; margin: auto"><img src="https://wx2.sinaimg.cn/large/d7b90c85ly1fww4hqied3j21kw0esjwt.jpg" alt="图2"></div></p>
<ul>
<li>Batch Normalization<ul>
<li><a href="../深度学习-Batch-Normalization深入解析/#为什么要进行归一化">为什么要进行归一化</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#简述 BN 的原理">简述 BN 的原理</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#BN 解决了什么问题">BN 解决了什么问题</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#使用 BN 有什么好处">使用 BN 有什么好处</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#BN 层通常处于网络中的什么位置">BN 层通常处于网络中的什么位置</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#BN 中 batch 的大小对网络性能有什么影响">BN 中 batch 的大小对网络性能有什么影响</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#BN 中线性偏移的参数个数怎么计算的">BN 中线性偏移的参数个数怎么计算的</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#BN 中使用的均值和方差是如何求得的">BN 中的使用的均值和方差是如何求得的</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#在多卡训练使用 BN 时, 需要注意什么问题">在多卡训练使用 BN 时, 需要注意什么问题</a></li>
<li><a href="../深度学习-Batch-Normalization深入解析/#使用 BN 时, 前一层的卷积网络需不需要偏置项, 为什么">使用 BN 时, 前一层的卷积网络需不需要偏置项, 为什么</a></li>
</ul>
</li>
<li>Group Normalization<ul>
<li><a href="../计算机视觉-GroupNormalization-ECCV2018/#简述 GN 的原理">简述 GN 的原理</a></li>
<li><a href="../计算机视觉-GroupNormalization-ECCV2018/#为什么 GN 效果好">为什么 GN 效果好</a></li>
<li><a href="../计算机视觉-GroupNormalization-ECCV2018/#简述 BN, LN, IN, GN 的区别">简述 BN, LN, IN, GN 的区别</a></li>
<li><a href="../计算机视觉-GroupNormalization-ECCV2018/#GN 中线性偏移的参数个数怎么计算的">GN 中线性偏移的参数个数怎么计算的</a></li>
</ul>
</li>
<li>Layer Normalization</li>
<li>Instance Normalization</li>
<li>Switchable Normalization</li>
</ul>
<p><span id="感受野"></span></p>
<h2 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h2><ul>
<li>感受野的作用</li>
<li>理论感受野和有效感受野的区别? <a href="https://mp.weixin.qq.com/s/R8rEngNI31w0DQwjjeS6kw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/R8rEngNI31w0DQwjjeS6kw</a></li>
<li>目标检测中的 anchor 的设置和感受野的大小之间有什么关系? <a href="https://mp.weixin.qq.com/s/R8rEngNI31w0DQwjjeS6kw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/R8rEngNI31w0DQwjjeS6kw</a></li>
</ul>
<p><span id="全连接层"></span></p>
<h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><ul>
<li>全连接层的作用是什么? <a href="https://www.zhihu.com/question/41037974" target="_blank" rel="noopener">https://www.zhihu.com/question/41037974</a></li>
<li>为什么要将全连接层转化为卷积层? <a href="https://www.cnblogs.com/liuzhan709/p/9356960.html" target="_blank" rel="noopener">https://www.cnblogs.com/liuzhan709/p/9356960.html</a></li>
<li>请推导全连接层的反向传播算法. <a href="https://zhuanlan.zhihu.com/p/39195266" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/39195266</a></li>
</ul>
<p><span id="卷积层"></span></p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>卷积层的作用?</p>
<ul>
<li><a href="../深度学习-各种网络层/#简述 1x1 卷积层的作用">简述 1x1 卷积层的作用</a></li>
<li><a href="../深度学习-各种网络层/#卷积操作的本质特性包括稀疏交互和参数共享, 具体解释这两种特性及其作用">卷积操作的本质特性包括稀疏交互和参数共享, 具体解释这两种特性及其作用</a></li>
<li><a href="../深度学习-各种网络层/#卷积层底层是如何实现的">卷积层底层是如何实现的</a></li>
<li><a href="../深度学习-各种网络层/#简述矩阵乘法的优化方法">简述矩阵乘法的优化方法</a></li>
</ul>
<p><strong>卷积操作通常由 GEMM 实现</strong>, 但是需要在内存进行名为 im2col 的初始重新排序, 以便将其映射到 GEMM 当中.</p>
<p>卷积核的大小如何确定?<br>卷积核的大小决定了该卷积核在上一层特征图谱上的感受野大小，在确定卷积核的大小时有以下原则（并非通用性原则，实际设计时需要结合具体情况决定）：在网络的起始层，选用较大的卷积核（7×7），这样可以使得卷积核“看到”更多的原图特征；在网络中中间层，可以用两个3×3大小的卷积层来代替一个5×5大小的卷积层，这样做可以在保持感受野大小不变的情况下降低参数个数，减少模型复杂度；通常使用奇数大小的卷积核，原因有二，一是可以更加方便的进行padding，二是奇数核相对于偶数核，具有天然的中心点，并且对边沿、对线条更加敏感，可以更有效的提取边沿信息</p>
<p><span id="反卷积层"></span></p>
<h2 id="反卷积层"><a href="#反卷积层" class="headerlink" title="反卷积层"></a>反卷积层</h2><p>反卷积与上采样.</p>
<ul>
<li>反卷积和双线性插值的区别, 各自的优势</li>
</ul>
<p><span id="池化层"></span></p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化层的作用?</p>
<ul>
<li>平均池化和最大池化有什么异同?</li>
<li>为什么用全局平均池化替换全连接层?</li>
</ul>
<p>讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？</p>
<p>个人理解: 最大池化层会保留核内的最大响应值, 也可以理解成是最显著的特征, 然后利用这些最显著的特征去预测, 从直觉上来说会取得较好的效果, 就像是人眼一样, 我们往往只通过一些很明显的特征就可以判断出一个物体的种类, 最大池化多多少少也有这一层含义.<br>而 mean pooling 是平均核内的所有特征, 这样做有一点不好的地方就是, 显著的响应值会被周围的不显著响应值所影响, 因此, 最终可能会得到一个不高不低的值. 举一个例子, 比如两处不同的位置进行 mean pooling, 一处的最大值是100, 然后经过mean pooling 之后, 它的输出值变成了 20, 而另一处的最大值是50, 然后经过mean pooling 之后它的输出值也是20, 这样, 对于不同的特征, 我们却得到了重复的结果, 这实际上是一种信息冗余, 也可以认为是一种特征丢失, 因此, 在使用中, maxpooling 更常用.</p>
<p>什么时候mean pooling 更好用? 通常在整个网络的最后, 我们会使用 gap 来整合整体的特征, 此时, 因为特征图谱已经是经过高度提取抽象后的, 所以, 我们不能只关注那些最大的值, 图谱上的每一个值所对应的特征我们都需要综合考虑, 因此, 我们通常会用 gap 来得到固定长度的特征向量, 进行最大的分类预测.</p>
<p>池化层的作用:<br>降低优化难度和参数个数：池化层可以降低特征图谱的维度，从而降低网络整体的复杂度，不仅可以加速计算，也能起到一定的防止过拟合的作用<br>增大感受野：当没有pooling时，一个3×3，步长为1的卷积，那么输出的一个像素的感受野就是3<em>3的区域，再加一个stride=1的3</em>3卷积，则感受野为5*5。当使用pooling后，很明显感受野迅速增大，这就是pooling的一大用处。感受野的增加对于模型的能力的提升是必要的，正所谓“一叶障目则不见泰山也”<br>平移不变性：pooling层只会关注核内的值，而不会关注该值的位置，因此，当目标位置发生移动时，pooling层也可以得到相同的结果，所以pooling层在一定程度上可以增加CNN网络的平移不变性</p>
<p><span id="训练问题"></span></p>
<h2 id="训练问题"><a href="#训练问题" class="headerlink" title="训练问题"></a>训练问题</h2><p><a href="../深度学习-训练问题">训练过程中遇到的问题及解决方案</a></p>
<ul>
<li><a href="../深度学习-训练问题/#在图像分类任务中, 训练数据不足会带来什么问题, 如何缓解数据量不足带来的问题?">在图像分类任务中, 训练数据不足会带来什么问题, 如何缓解数据量不足带来的问题?</a></li>
<li><a href="../深度学习-训练问题/#如何解决数据不均衡问题?">如何解决数据不均衡问题?</a></li>
<li><a href="../深度学习-训练问题/#训练不收敛的具体表现是什么? 可能的原因是什么? 如何解决?">训练不收敛的具体表现是什么? 可能的原因是什么? 如何解决?</a></li>
<li><a href="../深度学习-训练问题/#训练过程中出现 Nan 值是什么原因? 如何解决?">训练过程中出现 Nan 值是什么原因? 如何解决?</a></li>
<li><a href="../深度学习-训练问题/#过拟合是什么? 如何处理过拟合?">过拟合是什么? 如何处理过拟合?</a></li>
<li><a href="../深度学习-训练问题/#欠拟合是什么? 如何处理欠拟合?">欠拟合是什么? 如何处理欠拟合?</a></li>
<li><a href="../深度学习-训练问题/#Dropout 的实现方式在训练阶段和测试阶段有什么不同? 如何保持训练和测试阶段的一致性?">Dropout 的实现方式在训练阶段和测试阶段有什么不同? 如何保持训练和测试阶段的一致性?</a></li>
<li><a href="../深度学习-训练问题/#Dropout 为什么可以起到防止过拟合的作用?">Dropout 为什么可以起到防止过拟合的作用?</a></li>
<li><a href="../深度学习-训练问题/#如何选取 Batch Size 的值? 显存中通常会存储哪些东西?">如何选取 Batch Size 的值? 显存中通常会存储哪些东西?</a></li>
</ul>
<h1 id="网络结构篇"><a href="#网络结构篇" class="headerlink" title="网络结构篇"></a>网络结构篇</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">层数</th>
<th style="text-align:center">特点</th>
<th style="text-align:center">参数量</th>
<th style="text-align:center">Top-1 Acc</th>
<th style="text-align:center">Top-5 Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AlexNet</td>
<td style="text-align:center">8</td>
<td style="text-align:center"></td>
<td style="text-align:center">6000w+</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">VGGNet</td>
<td style="text-align:center">19</td>
<td style="text-align:center"></td>
<td style="text-align:center">1亿3000w+</td>
<td style="text-align:center">71.1</td>
<td style="text-align:center">89.8</td>
</tr>
<tr>
<td style="text-align:center">InceptionV1 (GoogLeNet)</td>
<td style="text-align:center">22</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">69.8</td>
<td style="text-align:center">89.6</td>
</tr>
<tr>
<td style="text-align:center">InceptionV2</td>
<td style="text-align:center">22</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">73.9</td>
<td style="text-align:center">91.8</td>
</tr>
<tr>
<td style="text-align:center">InceptionV3</td>
<td style="text-align:center">22</td>
<td style="text-align:center"></td>
<td style="text-align:center">2000w+</td>
<td style="text-align:center">78.0</td>
<td style="text-align:center">93.9</td>
</tr>
<tr>
<td style="text-align:center">InceptionV4</td>
<td style="text-align:center">22</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">80.2</td>
<td style="text-align:center">95.2</td>
</tr>
<tr>
<td style="text-align:center">ResNet</td>
<td style="text-align:center">152</td>
<td style="text-align:center"></td>
<td style="text-align:center">200w</td>
<td style="text-align:center">76.8</td>
<td style="text-align:center">93.2</td>
</tr>
<tr>
<td style="text-align:center">InceptionResNet</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">80.4</td>
<td style="text-align:center">95.3</td>
</tr>
<tr>
<td style="text-align:center">ResNeXt</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p><span id="AlexNet"></span></p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p><div style="width: 550px; margin: auto"><img src="https://wx2.sinaimg.cn/large/d7b90c85ly1g0ixnfk7w4j20o408q3yr.jpg" alt=""></div></p>
<p>AlexNet 的网络结构相对来说比较简单, 它包括 5 层卷积层, 3 层最大池化层, 以及 3 层全连接层. 池化层被分别放置在 conv1, conv2, 和 conv5 的后面. 虽然 AlexNet 结构简单, 但是由于全连接层的存在, 使得 AlexNet 的参数量较大, 大约有 6000w 个参数.</p>
<p><span id="VGGNet"></span></p>
<h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p><div style="width: 550px; margin: auto"><img src="https://wx4.sinaimg.cn/large/d7b90c85ly1fxmo2fpvavj21650u079w.jpg" alt=""></div></p>
<p>VGGNet 的网络结构延续了 AlexNet 的设计思想. 将卷积层分成 5 段, 每一段之间通过池化层分隔开, 后面同样跟了 3 层全连接层, 同时他用多个小卷积核替换了 AlexNet 中的大卷积核, 可以在减少参数量的同时提高感受野的范围, 并且通过统建更深层的网络, 使得提取到的特征图谱的表征能力更强. VGGNet 比较常用的结构有 VGG16 和 VGG19. 二者的区别在于前者每个卷积段的卷积层数量是(2, 2, 3, 3, 3), 后者每个卷积段中的卷积层数量是(2, 2, 4, 4, 4).</p>
<p><span id="InceptionV1"></span></p>
<h2 id="InceptionV1"><a href="#InceptionV1" class="headerlink" title="InceptionV1"></a><a href="../计算机视觉-InceptionV1">InceptionV1</a></h2><p>GoogLeNet 模型的核心思想是 <strong>卷积神经网络中的最优局部稀疏结构可以被现有的组件逼近和覆盖, 因此, 只要找到这个局部最优结构, 并在网络结构中重复使用它, 就可以进一步提升神经网络的拟合能力.</strong> 于是, InceptionV1 跳出了传统卷积神经网络的简单堆叠结构, 首次提出了 Inception 模块. Inception 模块综合了 1x1, 3x3, 5x5 这三种不同尺度的卷积核进行特征提取, 同时, 考虑到池化层的重要作用, 还综合了 3x3 大小的最大池化层. 并且, 还在 3x3 和 5x5 的卷积层之前, 以及池化层之后, 使用了 1x1 的卷积层来降低特征维度, 从而减少计算量. 以 Inception 模块为基本单元就可以构建出 IncetionV1 模型, 构建时仍然遵循了 5 个卷积段的段落形式, 段之间通过最大池化层分隔, 具体来说, 前两段使用的是传统的卷积层, 其中第一段是单层的 7x7 大小的卷积层, 第二段是两层较小尺寸的卷积层(1x1, 3x3), 后三段卷积段都是由 Inception 模块组成, 每一段使用的 Inception 模块数量分别为 2, 5, 2. 最后的分类层由全局平均池化层, 全连接层, Softmax 激活层组成. 另外, 由于网络结构较深, 因此, 为了防止梯度消失, InceptionV1 分别在 4a 和 4d 的 Inception 模块上添加了辅助侧枝分类器, 该分类器由一层平均池化层, 一层 1x1 卷积层, 两层全连接层和 Softmax 激活层组成.</p>
<ul>
<li><a href="../计算机视觉-InceptionV1/#简述一下 GoogLeNet 采用多个卷积核的原因">简述一下 GoogLeNet 采用多个卷积核的原因</a></li>
<li><a href="../计算机视觉-InceptionV1/#Inception 中为什么使用 1×1 卷积层">Inception 中为什么使用 1×1 卷积层</a></li>
<li><a href="../计算机视觉-InceptionV1/#Inception 中为什么使用全局平均池化层">Inception 中为什么使用全局平均池化层</a></li>
<li><a href="../计算机视觉-InceptionV1/#为什么使用侧枝">为什么使用侧枝</a></li>
<li><a href="../计算机视觉-InceptionV1/#GoogLeNet 在哪些地方使用了全连接层">GoogLeNet 在哪些地方使用了全连接层</a></li>
</ul>
<p><span id="InceptionV3"></span></p>
<h2 id="InceptionV3"><a href="#InceptionV3" class="headerlink" title="InceptionV3"></a><a href="../计算机视觉-InceptionV3">InceptionV3</a></h2><h3 id="简述-InceptionV2-相比于-GoogLeNet-有什么区别"><a href="#简述-InceptionV2-相比于-GoogLeNet-有什么区别" class="headerlink" title="简述 InceptionV2 相比于 GoogLeNet 有什么区别"></a>简述 InceptionV2 相比于 GoogLeNet 有什么区别</h3><p>InceptionV2 改进的主要有两点. 一方面加入了 BN 层, 减少了 Internal Covariate Shift 问题(内部网络层的数据分布发生变化), 另一方面参考了 VGGNet 用两个 $3\times 3$ 的卷积核替代了原来 Inception 模块中的 $5\times 5$ 卷积核, 可以在降低参数量的同时加速计算.</p>
<h3 id="简述-InceptionV3-相比于-GoogLeNet-有什么区别"><a href="#简述-InceptionV3-相比于-GoogLeNet-有什么区别" class="headerlink" title="简述 InceptionV3 相比于 GoogLeNet 有什么区别"></a>简述 InceptionV3 相比于 GoogLeNet 有什么区别</h3><p>InceptionV3 最重要的改进是分解(Factorization), 这样做的好处是既可以加速计算(多余的算力可以用来加深网络), 有可以将一个卷积层拆分成多个卷积层, 进一步加深网络深度, 增加神经网络的非线性拟合能力, 还有值得注意的地方是网络输入从 $224\times 224$ 变成了 $299\times 299$, 更加精细设计了 $35\times 35$, $17\times 17$, $8\times 8$ 特征图谱上的 Inception 模块.<br>具体来说, 首先将第一个卷积段的 $7\times 7$ 大小的卷积核分解成了 3 个 $3\times 3$ 大小的卷积核. 在第二个卷积段也由 3 个 $3\times 3$ 大小的卷积核组成. 第三个卷积段使用了 3 个 Inception 模块, 同时将模块中的 $5\times 5$ 卷积分解成了两个 $3\times 3$ 大小的卷积. 在第四个卷积段中, 使用了 5 个分解程度更高的 Inception 模块, 具体来说, 是将 $n\times n$ 大小的卷积核分解成 $1\times n$ 和 $n\times 1$ 大小的卷积核, 在论文中, 对于 $17\times 17$ 大小的特征图谱, 使用了 $n = 7$ 的卷积分解形式. 在第五个卷积段中, 面对 $8\times 8$ 大小的特征图谱, 使用了两个设计更加精细的 Inception 模块. 它将 $3\times 3$ 大小的卷积层分解成 $1\times 3$ 和 $3\times 1$ 的卷积层, 这两个卷积层不是之前的串联关系, 而是并联关系.</p>
<p><div style="width: 550px; margin: auto"><img src="https://wx2.sinaimg.cn/large/d7b90c85ly1g1g5nupi5fj21c80u0doq.jpg" alt="Inception"></div></p>
<ul>
<li><a href="..//#Inception 模块的设计和使用原则是什么">Inception 模块的设计和使用原则是什么</a></li>
</ul>
<p><span id="InceptionV4"></span></p>
<h2 id="InceptionV4-and-Inception-ResNet"><a href="#InceptionV4-and-Inception-ResNet" class="headerlink" title="InceptionV4 and Inception ResNet"></a><a href="../计算机视觉-InceptionV4-InceptionResNet">InceptionV4 and Inception ResNet</a></h2><p><strong>Inception 系列的缺点:</strong> 模块过于复杂, 人工设计的痕迹太重了.</p>
<h3 id="简述-InceptionV4-做了哪些改进"><a href="#简述-InceptionV4-做了哪些改进" class="headerlink" title="简述 InceptionV4 做了哪些改进"></a>简述 InceptionV4 做了哪些改进</h3><p>InceptionV4 使用了更复杂的结构重新设计了 Inception 模型中的每一个模块. 包括 Stem 模块, 三种不同的 Inception 模块以及两种不同的 Reduction 模块. 每一个模块的具体参数设置均不太一样, 但是整体来说都遵循的卷积分解和空间聚合的思想.</p>
<p><div style="width: 550px; margin: auto"><img src="https://wx2.sinaimg.cn/large/d7b90c85ly1g1g6on0w08j21hc0u0b29.jpg" alt="InceptionV4"></div></p>
<h3 id="简述-Inception-Resnet-v1-做了哪些改进"><a href="#简述-Inception-Resnet-v1-做了哪些改进" class="headerlink" title="简述 Inception-Resnet-v1 做了哪些改进"></a>简述 Inception-Resnet-v1 做了哪些改进</h3><p>Inception ResNet v1 网络主要被用来与 Inception v3 模型性能进行比较, 因此它所用的 Inception 子网络的计算相对常规模块有所减少, 这是为了保证使得它的整体计算和内存消耗与 Inception v3 近似, 如此才能保证公平性. 具体来说, Inception ResNet v1 网络主要讲 ResNet 中的残差思想用到了 Inception 模块当中, 对于每一种不太的 Inception 模块, 都添加了一个短接连接来发挥残差模型的优势.</p>
<p><div style="width: 550px; margin: auto"><img src="https://wx1.sinaimg.cn/large/d7b90c85ly1g1g6orzjmmj21hc0u0b29.jpg" alt="InceptionResNetV1"></div></p>
<h3 id="简述-Inception-ResNet-v2-做了哪些改进"><a href="#简述-Inception-ResNet-v2-做了哪些改进" class="headerlink" title="简述 Inception-ResNet-v2 做了哪些改进"></a>简述 Inception-ResNet-v2 做了哪些改进</h3><p>Inception ResNet v2 主要被设计来探索残差模块用于 Inception 网络时所尽可能带来的性能提升. 因此它是论文给出的最终性能最高的网络设计方案, 它和 Inception ResNet v1 的不同主要有两点, 第一是使用了 InceptionV4 中的更复杂的 Stem 结构, 第二是对于每一个 Inception 模块, 其空间聚合的维度都有所提升. 其模型结构如下所示:</p>
<p><div style="width: 550px; margin: auto"><img src="https://wx1.sinaimg.cn/large/d7b90c85ly1g1g6ox6512j21hc0u0e81.jpg" alt="InceptionResNetV2"></div></p>
<p><span id="Xception"></span></p>
<h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a><a href="">Xception</a></h2><p><span id="ResNet"></span></p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a><a href="../计算机视觉-ResNet-CVPR2016">ResNet</a></h2><ul>
<li><a href="../计算机视觉-ResNet-CVPR2016/#简述 ResNet 的原理">简述 ResNet 的原理</a></li>
<li><a href="../计算机视觉-ResNet-CVPR2016/#ResNet 中可以使用哪些短接方式">ResNet 中可以使用哪些短接方式</a></li>
<li><a href="../计算机视觉-ResNet-CVPR2016/#如何理解所谓的残差比原始目标更容易优化">如何理解所谓的残差 $F(x)$ 比原始目标 $H(x)$ 更容易优化</a></li>
<li><a href="../计算机视觉-ResNet-CVPR2016/#为什么恒等映射x之前的系数是1,而不是其他的值, 比如0.5">为什么恒等映射x之前的系数是1,而不是其他的值, 比如0.5</a></li>
<li><a href="../计算机视觉-ResNet-CVPR2016/#ResNet 到底解决了一个什么问题">ResNet 到底解决了一个什么问题</a></li>
<li><a href="../计算机视觉-ResNet-CVPR2016/#ResNet 残差模块中激活层应该如何放置">ResNet 残差模块中激活层应该如何放置</a></li>
</ul>
<p><span id="ResNeXt"></span></p>
<h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a><a href="../计算机视觉-ResNeXt-CVPR2017">ResNeXt</a></h2><ul>
<li><a href="../计算机视觉-ResNeXt-CVPR2017/#ResNeXt 在 ResNet 上做了哪些改进">ResNeXt 在 ResNet 上做了哪些改进</a></li>
</ul>
<p><div style="width: 550px; margin: auto"><img src="https://wx3.sinaimg.cn/large/d7b90c85ly1fxc7ewvfr1j20u00wn0y7.jpg" alt=""></div></p>
<p><div style="width: 550px; margin: auto"><img src="https://wx1.sinaimg.cn/large/d7b90c85ly1fxc7gghlzjj21xt0l1n26.jpg" alt=""></div></p>
<p><span id="DenseNet"></span></p>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a><a href="">DenseNet</a></h2><ul>
<li><a href="../计算机视觉-DenseNet-CVPR2017/#简述 DenseNet 的原理">简述 DenseNet 的原理</a></li>
</ul>
<p><span id="SqueezeNet"></span></p>
<h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a><a href="">SqueezeNet</a></h2><ul>
<li><a href="../计算机视觉-SqueezeNet/#简述 SqueezeNet 的原理">简述 SqueezeNet 的原理</a></li>
</ul>
<p><span id="MobileNet"></span></p>
<h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a><a href="../计算机视觉-MobileNet">MobileNet</a></h2><ul>
<li><a href="../计算机视觉-MobileNet/#简述 MobileNet 的原理">简述 MobileNet 的原理</a></li>
</ul>
<p><span id="MobileNetV2"></span></p>
<h2 id="MobileNetV2"><a href="#MobileNetV2" class="headerlink" title="MobileNetV2"></a><a href="">MobileNetV2</a></h2><ul>
<li><a href="../计算机视觉-MobileNetV2/#MobileNetV2 做了哪些改进">MobileNetV2 做了哪些改进</a></li>
</ul>
<p><span id="ShuffleNet"></span></p>
<h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a><a href="../计算机视觉-ShuffleNet">ShuffleNet</a></h2><ul>
<li><a href="../计算机视觉-ShuffleNet/#简述 ShuffleNet 的原理">简述 ShuffleNet 的原理</a></li>
<li><a href="../计算机视觉-ShuffleNet/简述 ShuffleNet 和 MobileNet 的区别">简述 ShuffleNet 和 MobileNet 的区别</a></li>
</ul>
<p><span id="ShuffleNetV2"></span></p>
<h2 id="ShuffleNetV2"><a href="#ShuffleNetV2" class="headerlink" title="ShuffleNetV2"></a><a href="">ShuffleNetV2</a></h2><ul>
<li><a href="..//#ShuffleNetV2 做了哪些改进">ShuffleNetV2 做了哪些改进</a></li>
</ul>
<p><span id="SENet"></span></p>
<h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a><a href="">SENet</a></h2><ul>
<li><a href="..//#简述 SENet 的原理">简述 SENet 的原理</a></li>
</ul>
<h1 id="目标检测篇"><a href="#目标检测篇" class="headerlink" title="目标检测篇"></a>目标检测篇</h1><p><span id="NMS"></span></p>
<h2 id="NMS"><a href="#NMS" class="headerlink" title="NMS"></a>NMS</h2><ul>
<li><a href="../计算机视觉-NMS-Implementation/#简述 NMS 的原理">简述 NMS 的原理</a></li>
<li><a href="../计算机视觉-NMS-Implementation/#NMS 算法源码实现">NMS 算法源码实现</a></li>
<li><a href="../计算机视觉-NMS-Implementation/#简述 Soft-NMS 的原理及算法实现">简述 Soft-NMS 的原理</a></li>
<li><a href="../计算机视觉-NMS-Implementation/#Soft-NMS 算法源码实现">Soft-NMS 算法源码实现</a></li>
<li><a href="../计算机视觉-NMS-Implementation/#介绍一下其他的 NMS 算法">介绍一下其他的 NMS 算法</a></li>
</ul>
<p><span id="R-CNN"></span></p>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="../计算机视觉-R-CNN-CVPR2014">R-CNN</a></h2><ul>
<li>Selective Search</li>
<li>AlexNet</li>
<li>SVM</li>
<li>Bounding Box Regression</li>
</ul>
<script type="math/tex; mode=display">t_x = (G_x - P_x) / P_w, t_y = (G_y - P_y) / P_h</script><script type="math/tex; mode=display">t_w = log(G_w / P_w), t_h = log(G_h / P_h)</script><p><span id="Fast R-CNN"></span></p>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="../计算机视觉-FastR-CNN-ICCV2015">Fast R-CNN</a></h2><p><strong>R-CNN 缺点</strong>:</p>
<ul>
<li>训练过程是分阶段的(Training is a multi-stage pipeline)</li>
<li>Training is expensive in space and time</li>
<li>目标检测速度太慢(Object detection is slow)</li>
</ul>
<p><strong>SPPNet 缺点</strong>:</p>
<ul>
<li>训练过程是分阶段的(Training is a multi-stage pipeline)</li>
<li>无法 Fine-Tuning 金字塔池化层之前的卷积层</li>
</ul>
<p><strong>Fast R-CNN 贡献</strong>:</p>
<ul>
<li>更高的检测准确率(mAP)</li>
<li>整个训练过程更加统一(利用多目标损失函数)</li>
<li>训练时可以对所有网络层参数进行更新(相比于SPPNet)</li>
<li>无需在硬盘上额外存储 feature.(相比于 R-CNN, 因为共享卷积计算结果, 使得feature的体积大大降低)</li>
</ul>
<p><strong>要点</strong>:</p>
<ul>
<li>Multi-task loss: 下式中, $L_{cls}(p,u) = - log p_u$, 即对于真实类别 $u$ 的 log 损失.<script type="math/tex; mode=display">L(p, u, t_u, v) = L_{cls}(p,u) + \lambda [u \geq 1] L_{loc}(t^u, v) \tag 1</script><script type="math/tex; mode=display">L_{loc}(t^u, v) = \sum_{i\in {x,y,w,h}} smooth_{L_1}(t_i^u - v_i) \tag 2</script><script type="math/tex; mode=display">smooth_{L_1}(x) = \begin{cases} 0.5x^2 && |x|<1 \\ |x| - 0.5 && otherwise \end{cases} \tag 3</script></li>
<li>Mini-batch Sampling</li>
<li>RoI Pooling Layer</li>
<li><a href="../深度学习-奇异值分解">Truncated SVD 截断式奇异矩阵分解</a></li>
</ul>
<p><span id="Faster R-CNN"></span></p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><a href="../计算机视觉-FasterR-CNN-NIPS2015">Faster R-CNN</a></h2><ul>
<li>RPN (Region Proposals Networks)<br><div style="width: 550px; margin: auto"><img src="https://wx1.sinaimg.cn/large/d7b90c85ly1fxsf9g1udsj21q00s3152.jpg" alt="图2"></div></li>
<li>损失函数<script type="math/tex; mode=display">L(\{p_i\}, \{t_i\}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^* ) + \lambda \frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^* )</script></li>
<li>RPN 与 Fast R-CNN 共享卷积参数<ul>
<li>Alternating training(交叉训练)</li>
<li>Approximate joint training(近似联合训练)</li>
<li>Non-approximate joint training(非近似联合训练)</li>
</ul>
</li>
</ul>
<p><strong>问题:</strong></p>
<p>(1). 为什么 Fast 和 Faster R-CNN 没有采用像 multi-scale testing (image pyramids)?</p>
<ul>
<li>FPN: 但是, 在 image pyramids 的每一层上提取特征具有很明显的缺点, 那就是会使得 Inference time 显著提升. 更进一步的, 会使得在 image pyramid 上训练端到端的深层网络变的不可行, 也因此, 我们仅仅在测试阶段才会使用 image pyramids, 这会在训练和测试阶段之间产生不一致问题. <strong>基于以上原因, Fast 和 Faster R-CNN 选择在默认设置下不使用 featurized image pyramids</strong></li>
</ul>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a><a href="../计算机视觉-MaskR-CNN-ICCV2017">Mask R-CNN</a></h2><ul>
<li>基于 Faster R-CNN 的基本结构, 替换 backbone 为 ResNet-50/101-C4 和 ResNet-50/101-FPN, ResNeXt-101-FPN.</li>
<li>除了边框回归和目标分类两个分支外, 新添加了一个全卷积的 Mask Prediction 网络, 该分支在每一个 RoI 上的输出维度为 $K\times m^2$, 代表 $K$ 个类别的二值掩膜.</li>
<li>在计算 mask 的损失时, 使用的是 sigmoid 的二分类损失, 而不是多分类的 softmax 损失, 这减少了类别之前的竞争, 使得可以生成更好的分割结果.</li>
<li>RoIAlign 消除了 RoI Pooling 中的两次量化操作, 使得最终提取到的特征可以 RoI 尽可能的对齐. 从而大幅提升在实例分割任务上的性能表现.</li>
</ul>
<p><div style="width: 550px; margin: auto"><img src="https://wx2.sinaimg.cn/large/d7b90c85ly1fx1toyw0xkj20kc0a5wkw.jpg" alt="图1"></div></p>
<h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a><a href="../计算机视觉-FPN-CVPR2017">FPN</a></h2><h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a><a href="../计算机视觉-FocalLoss-ICCV2017">RetinaNet</a></h2><p>FocalLoss</p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p>【链接】目标检测|SSD原理与实现<br><a href="https://zhuanlan.zhihu.com/p/33544892" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33544892</a></p>
<p><strong>问题:</strong></p>
<ul>
<li>为什么SSD不直接使用浅层的特征图谱, 而非要额外增加卷积层, 这样不是增加模型的复杂度了吗?</li>
<li>SSD 使用了哪些数据增广方法?</li>
</ul>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p><a href="http://caffecn.cn/?/question/1842" target="_blank" rel="noopener">http://caffecn.cn/?/question/1842</a></p>
<h3 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h3><h3 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h3><p>YOLOv2(也叫做 YOLO9000)</p>
<h3 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h3><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h1 id="图像处理篇"><a href="#图像处理篇" class="headerlink" title="图像处理篇"></a>图像处理篇</h1><p>4.1 图像特征提取的算法有哪些，各自优缺点、适用范围<br><a href="https://blog.csdn.net/xiongchao99/article/details/78776629" target="_blank" rel="noopener">https://blog.csdn.net/xiongchao99/article/details/78776629</a></p>
<p><span id="图像放缩"></span></p>
<h2 id="图像放缩"><a href="#图像放缩" class="headerlink" title="图像放缩"></a>图像放缩</h2><p><strong>双线性插值</strong><br>双线性插值本质上就是在两个方向上做线性插值.</p>
<p><div style="width: 550px; margin: auto"><img src="https://wx3.sinaimg.cn/large/d7b90c85ly1g1hbfdtyvcj21dg0m6ta3.jpg" alt="双线性插值"></div></p>
<p>假如我们想得到未知函数 $f$ 在点 $P = (x, y)$ 的值, <strong>在图像中, 这个 $f$ 代表的就是某个像素点的像素值.</strong> 当我们已知点 $P$ 周围四个点的值以后, 我们可以首先在 $x$ 方向上进行线性插值, 得到:</p>
<script type="math/tex; mode=display">f(R_1) \approx \frac{x_2 - x}{x_2 - x_1} f(Q_{11}) + \frac{x - x1}{x_2 - x_1}f(Q_{21})</script><script type="math/tex; mode=display">f(R_2) \approx \frac{x_2 - x}{x_2 - x_1} f(Q_{12}) + \frac{x - x_1}{x_2 - x_1} f(Q_{22})</script><p>然后在 $y$ 方向上进行线性插值, 得到:</p>
<script type="math/tex; mode=display">f(P) \approx \frac{y_2 - y}{y_2 - y_1} f(R_1) + \frac{y - y_1}{y_2 - y_1}f(R_2)</script><p><span id="边缘检测算法"></span></p>
<h2 id="边缘检测算法"><a href="#边缘检测算法" class="headerlink" title="边缘检测算法"></a>边缘检测算法</h2><p><a href="https://www.jianshu.com/p/2334bee37de5" target="_blank" rel="noopener">https://www.jianshu.com/p/2334bee37de5</a></p>
<p><a href="https://blog.csdn.net/KYJL888/article/details/78253053" target="_blank" rel="noopener">https://blog.csdn.net/KYJL888/article/details/78253053</a></p>
<p>Roberts算子<br>Sobel算子<br>Prewit算子<br>Canny算子</p>
<p><span id="霍夫变换"></span></p>
<h2 id="霍夫变换"><a href="#霍夫变换" class="headerlink" title="霍夫变换"></a>霍夫变换</h2><p><a href="https://www.cnblogs.com/AndyJee/p/3805594.html" target="_blank" rel="noopener">https://www.cnblogs.com/AndyJee/p/3805594.html</a></p>
<p><a href="https://blog.csdn.net/m0_37264397/article/details/72729423" target="_blank" rel="noopener">https://blog.csdn.net/m0_37264397/article/details/72729423</a></p>
<p><span id="滤波器"></span></p>
<h2 id="图像保边滤波器"><a href="#图像保边滤波器" class="headerlink" title="图像保边滤波器"></a>图像保边滤波器</h2><p><a href="https://blog.csdn.net/Trent1985/article/details/80509232" target="_blank" rel="noopener">https://blog.csdn.net/Trent1985/article/details/80509232</a></p>
<p><a href="https://blog.csdn.net/eejieyang/article/details/52333112" target="_blank" rel="noopener">https://blog.csdn.net/eejieyang/article/details/52333112</a></p>
<p><a href="https://blog.csdn.net/LG1259156776/article/details/51816875" target="_blank" rel="noopener">https://blog.csdn.net/LG1259156776/article/details/51816875</a></p>
<p><a href="https://blog.csdn.net/u012968002/article/details/44463229" target="_blank" rel="noopener">https://blog.csdn.net/u012968002/article/details/44463229</a></p>
<h2 id="图像平移"><a href="#图像平移" class="headerlink" title="图像平移"></a>图像平移</h2><p><a href="https://blog.csdn.net/qq_25867649/article/details/52131252" target="_blank" rel="noopener">https://blog.csdn.net/qq_25867649/article/details/52131252</a></p>
<p><a href="https://blog.csdn.net/linqianbi/article/details/78593203" target="_blank" rel="noopener">https://blog.csdn.net/linqianbi/article/details/78593203</a></p>
<h2 id="图像开操作、闭操作"><a href="#图像开操作、闭操作" class="headerlink" title="图像开操作、闭操作"></a>图像开操作、闭操作</h2><p><a href="https://blog.csdn.net/learning_tortosie/article/details/80030201" target="_blank" rel="noopener">https://blog.csdn.net/learning_tortosie/article/details/80030201</a></p>
<p><a href="https://blog.csdn.net/water_93/article/details/50859193" target="_blank" rel="noopener">https://blog.csdn.net/water_93/article/details/50859193</a></p>
<p><a href="https://www.cnblogs.com/daxiongblog/p/6289551.html" target="_blank" rel="noopener">https://www.cnblogs.com/daxiongblog/p/6289551.html</a></p>
<h2 id="图像旋转"><a href="#图像旋转" class="headerlink" title="图像旋转"></a>图像旋转</h2><p><a href="https://www.cnblogs.com/hustlx/p/5245226.html" target="_blank" rel="noopener">https://www.cnblogs.com/hustlx/p/5245226.html</a></p>
<p><a href="https://blog.csdn.net/ccblogger/article/details/72918354" target="_blank" rel="noopener">https://blog.csdn.net/ccblogger/article/details/72918354</a></p>
<h2 id="图像重建质量评价指标"><a href="#图像重建质量评价指标" class="headerlink" title="图像重建质量评价指标"></a>图像重建质量评价指标</h2><p><a href="https://blog.csdn.net/smallstones/article/details/42198049" target="_blank" rel="noopener">https://blog.csdn.net/smallstones/article/details/42198049</a></p>
<h2 id="光流法"><a href="#光流法" class="headerlink" title="光流法"></a>光流法</h2><p><a href="https://blog.csdn.net/longlovefilm/article/details/79824723" target="_blank" rel="noopener">https://blog.csdn.net/longlovefilm/article/details/79824723</a></p>
<p><a href="https://www.xuebuyuan.com/3203656.html" target="_blank" rel="noopener">https://www.xuebuyuan.com/3203656.html</a></p>
<h2 id="图像去噪的方法"><a href="#图像去噪的方法" class="headerlink" title="图像去噪的方法"></a>图像去噪的方法</h2><p><a href="https://blog.csdn.net/eric_e/article/details/79504444" target="_blank" rel="noopener">https://blog.csdn.net/eric_e/article/details/79504444</a></p>
<h2 id="度量图像patch相似度的方法"><a href="#度量图像patch相似度的方法" class="headerlink" title="度量图像patch相似度的方法"></a>度量图像patch相似度的方法</h2><p><a href="https://blog.csdn.net/lg1259156776/article/details/47037583/" target="_blank" rel="noopener">https://blog.csdn.net/lg1259156776/article/details/47037583/</a></p>
<p><a href="https://blog.csdn.net/zchang81/article/details/73275155/" target="_blank" rel="noopener">https://blog.csdn.net/zchang81/article/details/73275155/</a></p>
<p><a href="https://blog.csdn.net/yangyangyang20092010/article/details/8472257" target="_blank" rel="noopener">https://blog.csdn.net/yangyangyang20092010/article/details/8472257</a></p>
<h2 id="传统图像处理CDC做过吗？"><a href="#传统图像处理CDC做过吗？" class="headerlink" title="传统图像处理CDC做过吗？"></a>传统图像处理CDC做过吗？</h2><h2 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h2><h2 id="图像融合算法有哪些？"><a href="#图像融合算法有哪些？" class="headerlink" title="图像融合算法有哪些？"></a>图像融合算法有哪些？</h2><h2 id="图像增强算法有哪些"><a href="#图像增强算法有哪些" class="headerlink" title="图像增强算法有哪些"></a>图像增强算法有哪些</h2><h2 id="图像滤波方法"><a href="#图像滤波方法" class="headerlink" title="图像滤波方法"></a>图像滤波方法</h2><h2 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h2><p><a href="https://www.cnblogs.com/hustlx/p/5245461.html" target="_blank" rel="noopener">https://www.cnblogs.com/hustlx/p/5245461.html</a></p>
<p><a href="https://www.cnblogs.com/tianyalu/p/5687782.html" target="_blank" rel="noopener">https://www.cnblogs.com/tianyalu/p/5687782.html</a></p>
<h1 id="数学基础篇"><a href="#数学基础篇" class="headerlink" title="数学基础篇"></a>数学基础篇</h1><p><span id="概率分布"></span></p>
<h2 id="概率分布"><a href="#概率分布" class="headerlink" title="概率分布"></a>概率分布</h2><p><a href="../机器学习-数学基础/#Beta 分布">Beta 分布</a></p>
<p>5.1 概率分布<br>5.2 期望、方差、协方差、相关系数<br>5.3 假设检验<br><a href="https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/what-is-a-hypothesis-test/" target="_blank" rel="noopener">https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/what-is-a-hypothesis-test/</a></p>
<p><a href="https://blog.csdn.net/pipisorry/article/details/51182843" target="_blank" rel="noopener">https://blog.csdn.net/pipisorry/article/details/51182843</a></p>
<p><a href="https://blog.csdn.net/YtdxYHZ/article/details/51780310" target="_blank" rel="noopener">https://blog.csdn.net/YtdxYHZ/article/details/51780310</a></p>
<p>5.4 54张牌，分3组，大王小王同在一组的概率<br>分成3份 总的分法 M=(C54，18)(C36，18)(C18，18)</p>
<p>大小王在同一份N=(C3，1)(C52，16)(C36，18)x(C18，18) P=N /M=17/53 。</p>
<p>5.5 最大似然估计、贝叶斯估计<br><a href="https://blog.csdn.net/bitcarmanlee/article/details/52201858" target="_blank" rel="noopener">https://blog.csdn.net/bitcarmanlee/article/details/52201858</a></p>
<p><a href="https://www.cnblogs.com/zjh225901/p/7495505.html" target="_blank" rel="noopener">https://www.cnblogs.com/zjh225901/p/7495505.html</a></p>
<p><a href="https://blog.csdn.net/feilong_csdn/article/details/61633180" target="_blank" rel="noopener">https://blog.csdn.net/feilong_csdn/article/details/61633180</a></p>
<p>5.6<br><a href="https://www.nowcoder.com/questionTerminal/836b01b7809248b7b6e9c30495d4680e?from=14pdf" target="_blank" rel="noopener">https://www.nowcoder.com/questionTerminal/836b01b7809248b7b6e9c30495d4680e?from=14pdf</a></p>
<p>假设一段公路上，1小时内有汽车经过的概率为96%，那么，30分钟内有汽车经过的概率为?</p>
<p>48%<br>52%<br>80%<br>96%</p>
<p>一小时有车的概率 = 1 - 一小时没车的概率 = 1 - 两个半小时都没车的概率 = 1 - （1 - 半小时有车的概率）^2<br>1-(1-x)^2=0.96<br>x = 0.8</p>
<p>5.7 三门问题<br>三个宝箱里有一个宝箱里有宝物，两个是空的，你选了一个，主持人打开剩下2个中的一个发现没有宝物，问你换不换</p>
<p>假设A无，B无，C有，<br>选A，则主持人只会开B，1/3概率；<br>选B，则主持人只会开A，1/3概率；<br>选C，则主持人会开A\B，1/3概率；</p>
<p>可见，不换只有1/3的概率中，换的话，有2/3的概率中；</p>
<p>5.8 概率题：抛一个骰子，直到集齐六面，问抛骰子的期望次数。<br>5.9 概率题：抛色子连续n次正面向上的期望次数。<br>5.10 一个人向北走了一公里，向东走了一公里，又向南走了一公里，最后回到了最开始的起点，为什么？<br>南极点，刚好一个等边三角形；</p>
<p>或者是一个一圈距离刚好是1公里的那个地方。向北走之后，达到那个地方，饶了一圈回到这个地方，再向南走回去。</p>
<p><a href="https://blog.csdn.net/Turinglife/article/details/7358061" target="_blank" rel="noopener">https://blog.csdn.net/Turinglife/article/details/7358061</a></p>
<p>从逻辑上来讲，题目从好像缺少了一次向西的过程，才可以回到原地。有没有可能向东1公里还在原地，答案是肯定的，如果有一个纬度，绕其一圈恰好是1公里即可实现，所以这样的点有无穷多个，只要找到那个纬度即可。</p>
<p>5.11 一个四位数abcd，满足abcd <em> 4 = dcba，求这个数<br>a9没有进位，且为四位数，a只能为1<br>d9个位数为1，d只能是9<br>b9后为个位数（9加任何数进位），这个数只能是1或0，排除1，b=0<br>c9+8的尾数为0，则c</em>9个位数为2，c=8</p>
<p>a4没有进位，说明a=1或2，但是d4的个位是a，不可能a=1，所以a=2;</p>
<p>d=a4=8;而且没有进位，说明b4+它的可能进位不超过10；</p>
<p>如果b=0：则c*4的个位需要是7，不存在，不符；</p>
<p>如果b=1：则c*4的个位需要是8，存c=2不符合，c=7符合，所以为2178；</p>
<p>如果b=2:则c*4的个位需要是9，不符；</p>
<p>5.12 概率题：一个家庭有两个孩子，已知其中一个是女孩，求另一个也是女孩的概率<br>(1/21/2)/(1-1/21/2)<br>=(1/4)/(3/4)<br>=1/3</p>
<p>5.13 16个球队中随机选2个，在大量选取后，越强的队越容易被选中<br>5.14 有一个3L、一个5L的桶，请量出4L的水<br>5L桶装满水，倒入3L桶；此时5L中有2L水，3L桶中有3L水；</p>
<p>3L桶全部倒走，将5L桶的2L水道入3L桶中，此时5L桶中没有水，3L桶中有2L水；</p>
<p>将5L桶倒满水，然后向3L桶中倒水，此时3L桶水已满，5L桶中还剩4L水。</p>
<p>5.15 把1~9这9个数填入九格宫里,使每一横、竖、斜相等。<br><a href="https://zhidao.baidu.com/question/329122415632328485.html" target="_blank" rel="noopener">https://zhidao.baidu.com/question/329122415632328485.html</a></p>
<p>5.16 一个圆上随机三个点组成锐角三角形的概率<br>一个圆周上,随机点三个点,这三个点在同一个半圆上的概率是多少?<br>三个点在同一个半圆时,形成的三角形为直角或钝角三形（自己想为什么）.<br>不在同一个半圆时,为锐角三角形.<br>三点在同一半圆的概率是3/4,所以你这题的答案为1/4.</p>
<p>设在圆上任取的3点是A、B、C。圆心为 O<br>先假定A、B的位置，设弦AB的圆心角为∠α，且∠α属于[0，π].那么满足锐角三角形的C点就要在AO延长线与BO延长线间，所以C点的取值范围只有圆心为α的弧，即概率为：α/（2π）<br>对任意A、B的位置，C点的概率为对α/（2π）从[0，π]积分，结果是 π/4</p>
<p>关于为什么C点就要在AO延长线与BO延长线间，因为C点如果不在这之间，则ABC三点就会处于同一个半圆中。而处于同一个半圆中的三个点构成直角或者钝角三角形。</p>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><h1 id="常见问题篇"><a href="#常见问题篇" class="headerlink" title="常见问题篇"></a>常见问题篇</h1><p><a href="../面试-计算机视觉问题汇总">面试-计算机视觉问题汇总</a></p>
<ul>
<li>简述一下现在的 SOTA 目标检测模型</li>
<li>简要说一下 SSD, FPN, RefineDet, PFPNet, STDN, M2Det 等特征金字塔的区别?</li>
<li>你知道有哪些常用的训练 Trick?</li>
<li>知道哪些数据增广方法? 怎么实现的?</li>
<li>FCN 是如何降低计算量的?</li>
</ul>
<h1 id="未整理的问题"><a href="#未整理的问题" class="headerlink" title="未整理的问题"></a>未整理的问题</h1><p>ResNet 之后还有什么模型?</p>
<p>Batch 的 size 怎么选, 显存中一般会存储写什么: 显存占用 <a href="https://blog.csdn.net/lien0906/article/details/7886311" target="_blank" rel="noopener">https://blog.csdn.net/lien0906/article/details/7886311</a></p>
<p>【链接】网络inference阶段conv层和BN层的融合<br><a href="https://zhuanlan.zhihu.com/p/48005099" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48005099</a></p>
<p>模型压缩方法: SVD(由于近似计算, 会降低精度), Network Pruning, Crompression.</p>
<p>手写 iou, nms, soft-nms</p>
<p>二值交叉熵, softmax 公式</p>
<p>手写计算两向量的欧式距离</p>
<p>BN</p>
<p>链表排序</p>
<p>卷积参数量的计算</p>
<p>过拟合遇到过吗？怎么处理的</p>
<p>训练时出现 Nan 可能的原因是什么?  怎么办?<br><a href="https://blog.csdn.net/Michael__Corleone/article/details/78531795" target="_blank" rel="noopener">https://blog.csdn.net/Michael__Corleone/article/details/78531795</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/25110930" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25110930</a></p>
<p><a href="https://www.zhihu.com/question/49346370" target="_blank" rel="noopener">https://www.zhihu.com/question/49346370</a></p>
<p>PCA了解不，其优化目标是什么, Pca白化是什么？</p>
<p>手写BN的实现。注意BN的mean和std是在哪个维度求梯度的，mean和std是滑动平均的值。基于numpy实现</p>
<p>说下牛顿法</p>
<p>反卷积具体怎么实现的</p>
<p>为什么dropout能减少过拟合</p>
<p>NMS的原理，假设两个人靠的非常近，则会识别成一个bbx，会有什么问题，怎么解决</p>
<p>Pytorch当中permute和view的功能</p>
<h1 id="逻辑回归-1"><a href="#逻辑回归-1" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>Logistic(Sigmoid) 函数: $g(z) = \frac{1}{1+e^{-z}}$<br>Logistic 表达式: $h_\theta (x) = g(\theta^T x) = \frac{1}{1 + e^{-\theta^Tx}}$<br>线性回归模型: $\theta^T x = \sum^n_{i=1}\theta_i x_i$<br>SGD</p>
<p>Logistic 的梯度更新表达式和最小二乘法(LMS)的公式相同, 虽然看上去完全相同, 但实际上 SGD 和 LMS 是两个完全不同的算法, 因为 SGD 中的 $h_\theta (x)$ 表示的是关于 $\theta^T x$ 的一个非线性函数.</p>
<p>机器学习500问 第二章<br>百面机器学习 第三章</p>
<h1 id="线性回归和逻辑回归的原理和区别"><a href="#线性回归和逻辑回归的原理和区别" class="headerlink" title="线性回归和逻辑回归的原理和区别"></a>线性回归和逻辑回归的原理和区别</h1><h1 id="在Faster-RCNN中-如果两个物体重合度很高-会怎么样"><a href="#在Faster-RCNN中-如果两个物体重合度很高-会怎么样" class="headerlink" title="在Faster RCNN中, 如果两个物体重合度很高, 会怎么样"></a>在Faster RCNN中, 如果两个物体重合度很高, 会怎么样</h1><p>由于 Faster RCNN 在提取感兴趣区域的时候, 它的类别默认是只有二类, 即是否包含物体, 所以如果两个物体重合度很高的话, 最终可能就只能检测出一个物体.(right?)</p>
<h1 id="卷积神经网络复杂度分析"><a href="#卷积神经网络复杂度分析" class="headerlink" title="卷积神经网络复杂度分析"></a>卷积神经网络复杂度分析</h1><p><a href="../深度学习-卷积神经网络复杂度分析">卷积神经网络复杂度分析</a></p>
<h1 id="卷积计算-卷积层特参数个数及征图谱尺寸计算"><a href="#卷积计算-卷积层特参数个数及征图谱尺寸计算" class="headerlink" title="卷积计算,卷积层特参数个数及征图谱尺寸计算"></a>卷积计算,卷积层特参数个数及征图谱尺寸计算</h1><p>卷积层输入图谱大小为 $D_in \times D_{in} \times depth_{in}$ , 卷积核尺寸为 $F \times F \times depth_{in}$, 步长为 $stride$ ,结合padding,输出的图谱size是多少</p>
<script type="math/tex; mode=display">D_{out} = \frac{D_{in} - F + 2*Padding}{stride} + 1</script><p>输出的特征图谱的深度为卷积核的个数: $depth_{out} = Num_{filters}$</p>
<p>本层的偏置参数数量: $Num_{bias} = Num_{filters}$, 注意只与卷积核的个数有关, 与输入的特征图谱的深度无关</p>
<p>该层的参数个数 = 卷积核参数个数 + 偏置项参数个数:</p>
<script type="math/tex; mode=display">Num_{params} = F \times F \times depth_{in} \times depth_{out} + Num_{bias}</script><h1 id="L2正则化和L2规范化-归一化-的不同"><a href="#L2正则化和L2规范化-归一化-的不同" class="headerlink" title="L2正则化和L2规范化(归一化)的不同"></a>L2正则化和L2规范化(归一化)的不同</h1><p>正则化是指正则项, 计算完以后是一个矢量. 归一化是将向量中每个元素进行归一化, 计算完以后还是同size的向量, L2归一化实际上就是对每一个元素除以L2正则项.</p>
<h1 id="从-rcnn-到-faster"><a href="#从-rcnn-到-faster" class="headerlink" title="从 rcnn 到 faster"></a>从 rcnn 到 faster</h1><p><a href="https://blog.csdn.net/xiaoye5606/article/details/71191429" target="_blank" rel="noopener">https://blog.csdn.net/xiaoye5606/article/details/71191429</a></p>
<h1 id="为什么fast-rcnn的roi-pooling比spp的-spatial-pooling效果好"><a href="#为什么fast-rcnn的roi-pooling比spp的-spatial-pooling效果好" class="headerlink" title="为什么fast rcnn的roi pooling比spp的 spatial pooling效果好???"></a>为什么fast rcnn的roi pooling比spp的 spatial pooling效果好???</h1><h1 id="神经网络参数初始化"><a href="#神经网络参数初始化" class="headerlink" title="神经网络参数初始化"></a>神经网络参数初始化</h1><h1 id="分类问题为什么用交叉熵"><a href="#分类问题为什么用交叉熵" class="headerlink" title="分类问题为什么用交叉熵"></a>分类问题为什么用交叉熵</h1><p><strong>典型错误答案1:</strong> 如果用交叉熵，能保证神经网络训练时是一个凸优化问题</p>
<p>错误原因: 凸函数的复合并不一定是凸函数</p>
<p><strong>典型错误答案2:</strong> 如果当前值与目标值相差很远，则梯度下降法迭代时收敛的更快一些</p>
<p>错误原因: 欧式距离(平方损失)也能起到这个作用, 为什么不用?</p>
<p><strong>正确答案:</strong></p>
<p>Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison.</p>
<h1 id="Relu和Dropout都具有正则化作用-它们在正则化方面的区别是什么"><a href="#Relu和Dropout都具有正则化作用-它们在正则化方面的区别是什么" class="headerlink" title="Relu和Dropout都具有正则化作用, 它们在正则化方面的区别是什么?"></a>Relu和Dropout都具有正则化作用, 它们在正则化方面的区别是什么?</h1><p>Relu是强制正则化(所有神经元的输出值, 只要小于0, 就置为0)</p>
<p>Dropout是随机正则化(随机让一些神经元的不起作用)</p>
<h1 id="介绍一下-hard-negative-mining-难样例挖掘"><a href="#介绍一下-hard-negative-mining-难样例挖掘" class="headerlink" title="介绍一下 hard negative mining(难样例挖掘)"></a>介绍一下 hard negative mining(难样例挖掘)</h1><h1 id="dropout内部是怎么实现的"><a href="#dropout内部是怎么实现的" class="headerlink" title="dropout内部是怎么实现的"></a>dropout内部是怎么实现的</h1><p>在 <strong>训练阶段</strong> 给每个神经元的参数都会乘以 $\frac{1}{\alpha_{dropout}}$, 这样一来, 在训练阶段可以随时更改dropout的参数值, 而对于测试阶段来说, 无需对神经元进行任何额外处理, 所有的神经元都相当于适配了训练过程中dropout对参数带来的影响.</p>
<h1 id="简述一下BN"><a href="#简述一下BN" class="headerlink" title="简述一下BN"></a>简述一下BN</h1><p>首先标准化就是将数据归一到一个希望的区间内, 一般都是归一化到激活函数敏感区域内, 而BN和传统标准标准化的区别主要有两点:</p>
<ul>
<li>BN是在每一个batch上做标准化的, 并且不仅仅只对输入层数据做标准化, 对网络内部的隐藏层输入也会进行标准话</li>
<li>第二就是BN并不是在标准的减均值初标准差之后, 还会进行一个线性变换,<strong>其本质就是改变数据分布的方差和均值</strong>. 对应的两个参数是通过学习学出来的. 其主要思想是考虑到数据可能本身就具有一定的不对称性, 并且激活函数也不一定就在面对标准数据时才有最好的表现, 因此</li>
</ul>
<p>关于BN的详细解析可以看:</p>
<p><div style="width: 550px; margin: auto"><img src="Batch-Normalization深入解析" alt=""></div></p>
<h1 id="各种初始化方式，及公式各个参数对训练的影响"><a href="#各种初始化方式，及公式各个参数对训练的影响" class="headerlink" title="各种初始化方式，及公式各个参数对训练的影响"></a>各种初始化方式，及公式各个参数对训练的影响</h1><h1 id="目标检测，数据不平衡问题怎么解决"><a href="#目标检测，数据不平衡问题怎么解决" class="headerlink" title="目标检测，数据不平衡问题怎么解决"></a>目标检测，数据不平衡问题怎么解决</h1><p>对于目标物的不平衡问题, 通过采样方法来缓解.</p>
<p>对于前后景样本数的不平衡问题, 尝试使用FocalLoss来解决</p>
<script type="math/tex; mode=display">L = -(1-p_t)^\gamma log(p_t)</script><h1 id="你的zerotensor和TF比性能上有优势吗"><a href="#你的zerotensor和TF比性能上有优势吗" class="headerlink" title="你的zerotensor和TF比性能上有优势吗"></a>你的zerotensor和TF比性能上有优势吗</h1><h1 id="常用的数据增强技术"><a href="#常用的数据增强技术" class="headerlink" title="常用的数据增强技术"></a>常用的数据增强技术</h1><p>水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转</p>
<h1 id="有哪些可以避免过拟合的办法"><a href="#有哪些可以避免过拟合的办法" class="headerlink" title="有哪些可以避免过拟合的办法"></a>有哪些可以避免过拟合的办法</h1><p>数据增强, 正则化, 模型融合(其中dropout是模型融合方法中最高效和常用的技巧)</p>
<p>为了防止过拟合，增加训练样本是一个好的解决方案。此外，还可使用数据增强、L1 正则化、L2 正则化、Dropout、DropConnect 和早停（Early stopping）法等</p>
<h1 id="正则化L1和L2的区别"><a href="#正则化L1和L2的区别" class="headerlink" title="正则化L1和L2的区别"></a>正则化L1和L2的区别</h1><h2 id="rcnn。。"><a href="#rcnn。。" class="headerlink" title="rcnn。。"></a>rcnn。。</h2><h1 id="推导svm"><a href="#推导svm" class="headerlink" title="推导svm"></a>推导svm</h1><h1 id="卷积层的参数个数计算公式是："><a href="#卷积层的参数个数计算公式是：" class="headerlink" title="卷积层的参数个数计算公式是："></a>卷积层的参数个数计算公式是：</h1><p>输入的filers×kernerl size ×输出的filters。如：</p>
<p>（3×3×256）×512   括号前面是每一个卷积核的大小，后面的是总共有512个卷积核</p>
<h1 id="梯度消失和梯度爆炸的概念，引用这两个现象的原因及其解决办法"><a href="#梯度消失和梯度爆炸的概念，引用这两个现象的原因及其解决办法" class="headerlink" title="梯度消失和梯度爆炸的概念，引用这两个现象的原因及其解决办法"></a>梯度消失和梯度爆炸的概念，引用这两个现象的原因及其解决办法</h1><p>详见<a href="">梯度消失和梯度爆炸问题深入解析</a></p>
<p><span id="activation"> </span></p>
<h1 id="关于各种激活函数的解析与讨论"><a href="#关于各种激活函数的解析与讨论" class="headerlink" title="关于各种激活函数的解析与讨论"></a>关于各种激活函数的解析与讨论</h1><h1 id="简述ResNet"><a href="#简述ResNet" class="headerlink" title="简述ResNet"></a>简述ResNet</h1><h1 id="嵌入式开发很底层-一般还是倾向于做一些上层的东西"><a href="#嵌入式开发很底层-一般还是倾向于做一些上层的东西" class="headerlink" title="嵌入式开发很底层   一般还是倾向于做一些上层的东西"></a>嵌入式开发很底层   一般还是倾向于做一些上层的东西</h1><h1 id="推导SVM"><a href="#推导SVM" class="headerlink" title="推导SVM"></a>推导SVM</h1><h1 id="比较Boosting和Bagging的异同"><a href="#比较Boosting和Bagging的异同" class="headerlink" title="比较Boosting和Bagging的异同"></a>比较Boosting和Bagging的异同</h1><p>二者都是集成学习方法, 都是将多个弱学习器组合成强学习器的方法, 它们的区别在于:</p>
<p>Boosting: 每一轮根据上一轮的分类结果动态调整每个样本在分类器中的权重, 训练得到k个弱分类器, 他们都有各自的权重, 通过加权组合的方式得到最终的分类结果</p>
<p>Bagging: 从原始数据集中每一轮又放回地抽取训练集(抽取的训练集小于原始数据集), 训练得到k个弱学习器, 然后将这k个软学习器的分类结果结合, 得到最终的分类结果.</p>
<h1 id="无监督学习中存在过拟合吗"><a href="#无监督学习中存在过拟合吗" class="headerlink" title="无监督学习中存在过拟合吗?"></a>无监督学习中存在过拟合吗?</h1><p>存在.<br>//TODO 补充  什么情况下会产生无监督的过拟合</p>
<h1 id="什么是K折交叉验证"><a href="#什么是K折交叉验证" class="headerlink" title="什么是K折交叉验证?"></a>什么是K折交叉验证?</h1><p>将原始数据集划分为k个子集, 将其中一个子集作为验证集, 其余k-1个子集作为训练集, 如此训练和验证一轮成为一次交叉验证. 交叉验证重复k此, 每个子集都会做一次验证, 最终得到k个模型, 然后可以对这k个模型的结果加权平均, 以作为评估整体模型的依据</p>
<h1 id="关于k折交叉验证-需要注意什么"><a href="#关于k折交叉验证-需要注意什么" class="headerlink" title="关于k折交叉验证, 需要注意什么?"></a>关于k折交叉验证, 需要注意什么?</h1><p>k越大, 不一定效果越好, 而且越大的k会加大训练时间;</p>
<p>在选择k时, 需要考虑最小化数据集之间的方差, 比如对于2分类任务, 如果采用2折交叉验证, 即对原始数据集二分,若此时训练集中都是A类别, 验证集中都是B类别, 则交叉验证效果会非常差</p>
<h1 id="对于一个二分类问题-我们定义超过阈值t的判定为正例-否则判定为负例-现在若将t增大-则准确率和召回率会如何变化"><a href="#对于一个二分类问题-我们定义超过阈值t的判定为正例-否则判定为负例-现在若将t增大-则准确率和召回率会如何变化" class="headerlink" title="对于一个二分类问题, 我们定义超过阈值t的判定为正例, 否则判定为负例. 现在若将t增大, 则准确率和召回率会如何变化?"></a>对于一个二分类问题, 我们定义超过阈值t的判定为正例, 否则判定为负例. 现在若将t增大, 则准确率和召回率会如何变化?</h1><p>准确率 = TP / (TP + FP), 召回率 = TP / (TP, FN)</p>
<p>若增大阈值t, 则更多不确定的样本将会被分为负例, 剩余确定样本的所占比例会增大, 那么准确率就会提升(或不变); 同时, 由于那些不确定的样本中还可能包含有正例, 引起, 阈值调大后, 这些正例就会被认为是负例, 所以召回率减小(或不变)</p>
<h1 id="增加网络层数-是否总能减小训练集错误率"><a href="#增加网络层数-是否总能减小训练集错误率" class="headerlink" title="增加网络层数, 是否总能减小训练集错误率?"></a>增加网络层数, 是否总能减小训练集错误率?</h1><p>不能, 有时候网络层数过深, 还会因为梯度消失导致模型退化, 使得模型性能降低</p>
<h1 id="在目标检测问题上-如何做数据增广"><a href="#在目标检测问题上-如何做数据增广" class="headerlink" title="在目标检测问题上, 如何做数据增广?"></a>在目标检测问题上, 如何做数据增广?</h1><h1 id="softmax怎么跟交叉熵损失函数结合"><a href="#softmax怎么跟交叉熵损失函数结合" class="headerlink" title="softmax怎么跟交叉熵损失函数结合?"></a>softmax怎么跟交叉熵损失函数结合?</h1><h1 id="用梯度下降训练神经网络的参数-为什么参数有时候会被训练为nan值"><a href="#用梯度下降训练神经网络的参数-为什么参数有时候会被训练为nan值" class="headerlink" title="用梯度下降训练神经网络的参数, 为什么参数有时候会被训练为nan值?"></a>用梯度下降训练神经网络的参数, 为什么参数有时候会被训练为nan值?</h1><p>输入数据本身存在nan值, 或者考虑是否梯度爆炸了(可以试着降低学习率, 或者利用截断法先知梯度的值)</p>
<h1 id="有没有自己试过更改模型的框架"><a href="#有没有自己试过更改模型的框架" class="headerlink" title="有没有自己试过更改模型的框架."></a>有没有自己试过更改模型的框架.</h1><p>有时候读paper会遇到一些好的点子或者方法, 自己会去加到现有的网络中去验证一下是不是能够提升模型的性能, 一般情况下, 比较经典且认可度较高的一些算法, 由于在网上都能找到相应的源码, 加上去的时候性能往往会有一点提升, 但是有时候有的方法比较偏, 我加完了以后有时候是没作用, 有时候是性能降低了, 我不知道到底是我实现的和paper有出入, 还是这个东西不适合当前框架</p>
<h1 id="说一下你所有的提高精度的方法-并且说明它们带来了多少的精度提升"><a href="#说一下你所有的提高精度的方法-并且说明它们带来了多少的精度提升" class="headerlink" title="说一下你所有的提高精度的方法, 并且说明它们带来了多少的精度提升!"></a>说一下你所有的提高精度的方法, 并且说明它们带来了多少的精度提升!</h1><p>OHEM ~3%</p>
<h1 id="SSD已经使用了难样例挖掘的技巧-Focal-Loss-相比之下为什么能够提高"><a href="#SSD已经使用了难样例挖掘的技巧-Focal-Loss-相比之下为什么能够提高" class="headerlink" title="SSD已经使用了难样例挖掘的技巧, Focal Loss 相比之下为什么能够提高"></a>SSD已经使用了难样例挖掘的技巧, Focal Loss 相比之下为什么能够提高</h1><h1 id="池化的优点-优化的缺点"><a href="#池化的优点-优化的缺点" class="headerlink" title="池化的优点, 优化的缺点"></a>池化的优点, 优化的缺点</h1><p>优点:</p>
<ul>
<li>显著减少参数数量, 降低过拟合</li>
<li>池化单元具有平移不变性</li>
</ul>
<p>缺点:<br>pooling能够增大感受野, 让后续的卷积看到更多的信息, 但是它在降维的过程中丢失了一些信息, 这对segmentation要求的精确location有一定的影响, 所以pooling层跟segmentation有一定的冲突, 但是感受野的增大有可以特征检测实例的准确率, 还可以降低计算量, 增强泛化能力. 所以这个是实例分割问题需要解决的一个关键点之一.</p>
<h1 id="待定"><a href="#待定" class="headerlink" title="待定"></a>待定</h1><p><a href="https://blog.csdn.net/comway_Li/article/details/82532573" target="_blank" rel="noopener">https://blog.csdn.net/comway_Li/article/details/82532573</a></p>
<h1 id="完善bisai待看"><a href="#完善bisai待看" class="headerlink" title="完善bisai待看"></a>完善bisai待看</h1><p><a href="https://www.kaggle.com/c/google-ai-open-images-object-detection-track/discussion/64986" target="_blank" rel="noopener">https://www.kaggle.com/c/google-ai-open-images-object-detection-track/discussion/64986</a></p>
<p><a href="https://arxiv.org/abs/1809.00778" target="_blank" rel="noopener">https://arxiv.org/abs/1809.00778</a></p>
<p><a href="https://www.kaggle.com/c/google-ai-open-images-object-detection-track/discussion" target="_blank" rel="noopener">https://www.kaggle.com/c/google-ai-open-images-object-detection-track/discussion</a></p>
<h1 id="谈谈你参加的比赛"><a href="#谈谈你参加的比赛" class="headerlink" title="谈谈你参加的比赛"></a>谈谈你参加的比赛</h1><p>对于一个比赛任务, 我会首先进行预处理,  之后, 会根据数据集的数据分布来对参数进行调整, 比如, 先只训练顶层, 然后逐步放开, 最后再训练所有层的参数.  在训练的时候,我一般都会采用bagging的思想, 将训练集随机28分, 分成3份, 然后训练, 最后进行模型融合,  融合的时候我一般都是对训练结果进行融合.</p>
<p>如果是目标检测累任务, 那么就:…</p>
<p>如果是实力分割类任务, 那么就:</p>
<h1 id="对于一个新任务-你一般都会使用那些数据预处理方法"><a href="#对于一个新任务-你一般都会使用那些数据预处理方法" class="headerlink" title="(对于一个新任务,) 你一般都会使用那些数据预处理方法"></a>(对于一个新任务,) 你一般都会使用那些数据预处理方法</h1><ol>
<li>训练数据可视化</li>
</ol>
<p>首先, 不论是什么样的数据集, 我都会先随机挑选 20 到 100张 的训练数据, 然后根据标签, 将图片数据可视化出来, 比如说如果是目标检测的任务, 我就会用opencv 的<code>cv2.rectangle()</code> 函数和 <code>cv2.putText()</code> 函数将标签里面的bbox标签和类别标签画到图片上去, 并且建立一个字典结构, 将不同的class-id对应到不同的颜色,  如果是实力分割任务, 我就会将mask标签反应到图片上去, 一般就是先将单通道的mask扩展成多通道的, 同时根据不同的class-id赋予不同的颜色, 最后利用numpy的where方法和原始图片进行叠加.  一般对于这种几十张的smaple图片, 我都是直接保存, 这样以后想再看的时候也不用重新跑脚本了.</p>
<p>之后, 我就会先简单浏览一下这些数据, 对整个数据集有一个初步的把握, 大概知道哪些物体被标注了, 有时候也能发现很多标注存在问题, 不过这也没有办法,  毕竟标注是一个很费时费力的工作, 错误在所难免.</p>
<ol>
<li>计算数据分布信息</li>
</ol>
<p>然后我就会写个脚本对整个数据集和标签进行遍历, 统计一些信息, 通常我会检测这么几个信息:</p>
<p>图像的平均尺寸, 整个数据集的像素平均值, 每张图片平均包含的目标个数, 每个类别的目标个数以及目标的平均大小,</p>
<p>同时, 因为平均值有时候往往反应不出来太多信息, 所以我还会用matplotlib把每种信息的直方图画出来, 然后看一下数据的整体分布是什么样子的, 比如图片size的分布, 目标大小的分布等等, 我主要就是根据这些分布信息来决定我最开始的参数设置.  主要调的参数就是imagesize,anchors相关的参数, 其他的还有就非极大抑制和置信度的阈值, 有时候还会试一下BN的作用(默认是关闭的)</p>
<p>然后一般情况下我都会对数据集做增广</p>
<p>常用的就是裁剪, 反转, 虚化, 颜色变换等等, 增广我不会做太多, 一般就用一些常用的增广方法</p>
<h1 id="比赛中用到的模型融合方法"><a href="#比赛中用到的模型融合方法" class="headerlink" title="比赛中用到的模型融合方法:"></a>比赛中用到的模型融合方法:</h1><p>对于目标检测任务:</p>
<p>我用的融合策略就是先以一个结果文件为基准, 然后用另一个结果文件里面的某张图片的框去跟前一个结果文件对应图片的所有框作比较, 因为之间会对框的面积做排序, 所以只与面积相似的框作比较, 看看框的位置是不是也相似, 如果相似, 就认为检测的是同一个物体, 然后就看他们的类别是否相同, 这里我一般会使用三个结果文件(来自于三个不同模型)进行投票选择.</p>
<p>对于有的框不在另一个文件的,  我就会根据框的置信度来设置一个阈值, 大于阈值的我就直接把框加进去, 如果有票数相同的, 就按置信度来区分.</p>
<h1 id="你的方法与其他人方法的区别是什么-为什么比别人的方法差"><a href="#你的方法与其他人方法的区别是什么-为什么比别人的方法差" class="headerlink" title="你的方法与其他人方法的区别是什么? 为什么比别人的方法差?"></a>你的方法与其他人方法的区别是什么? 为什么比别人的方法差?</h1><h1 id="对于faster-rcnn-你都调了哪些参数"><a href="#对于faster-rcnn-你都调了哪些参数" class="headerlink" title="对于faster rcnn 你都调了哪些参数?"></a>对于faster rcnn 你都调了哪些参数?</h1><p>首先调的是anchor相关参数, 比如anchor size 和 anchor ratio</p>
<p>然后是学习率, 前景后景的样本比例, 非极大抑制的阈值, 候选区域块的生成个数, 图片的缩放尺度等等</p>
<h1 id="BN具体是什么实现的"><a href="#BN具体是什么实现的" class="headerlink" title="BN具体是什么实现的"></a>BN具体是什么实现的</h1><h1 id="对于平均移动了解吗"><a href="#对于平均移动了解吗" class="headerlink" title="对于平均移动了解吗"></a>对于平均移动了解吗</h1><h1 id="积分图-快速求矩阵的核"><a href="#积分图-快速求矩阵的核" class="headerlink" title="积分图, 快速求矩阵的核"></a>积分图, 快速求矩阵的核</h1><h1 id="样本不均衡问题怎么解决"><a href="#样本不均衡问题怎么解决" class="headerlink" title="样本不均衡问题怎么解决"></a>样本不均衡问题怎么解决</h1><h1 id="详细说一下Focal-Loss"><a href="#详细说一下Focal-Loss" class="headerlink" title="详细说一下Focal Loss"></a>详细说一下Focal Loss</h1><h1 id="说一下为什么Faster-比YOLO和SSD更准确"><a href="#说一下为什么Faster-比YOLO和SSD更准确" class="headerlink" title="说一下为什么Faster 比YOLO和SSD更准确"></a>说一下为什么Faster 比YOLO和SSD更准确</h1><h1 id="样本采样的理论化值是"><a href="#样本采样的理论化值是" class="headerlink" title="样本采样的理论化值是"></a>样本采样的理论化值是</h1><h1 id="anchor的参数设值怎么选的？-为什么这么设置"><a href="#anchor的参数设值怎么选的？-为什么这么设置" class="headerlink" title="anchor的参数设值怎么选的？  为什么这么设置"></a>anchor的参数设值怎么选的？  为什么这么设置</h1><h1 id="在调试RPN网络时有没有遇到什么问题？"><a href="#在调试RPN网络时有没有遇到什么问题？" class="headerlink" title="在调试RPN网络时有没有遇到什么问题？"></a>在调试RPN网络时有没有遇到什么问题？</h1><h1 id="简述一下faster-rcnn模型"><a href="#简述一下faster-rcnn模型" class="headerlink" title="简述一下faster rcnn模型"></a>简述一下faster rcnn模型</h1><h1 id="简述一下ResNet模型及它解决的问题"><a href="#简述一下ResNet模型及它解决的问题" class="headerlink" title="简述一下ResNet模型及它解决的问题"></a>简述一下ResNet模型及它解决的问题</h1>
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/知识点梳理/" rel="tag"><i class="fa fa-tag"></i> 知识点梳理</a>
          
            <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/z_post/计算机视觉-目标检测训练策略/" rel="prev" title="计算机视觉-目标检测训练策略">
                <i class="fa fa-chevron-left"></i> 计算机视觉-目标检测训练策略
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/z_post/面试-算法刷题-LeetCode-record/" rel="next" title="LeetCode算法题(记录总结)">
                LeetCode算法题(记录总结) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar_zz.png"
                alt="ZeroZone" />
            
              <p class="site-author-name" itemprop="name">ZeroZone</p>
              <p class="site-description motion-element" itemprop="description">并不是什么厉害的地方<br>只是一个安静的学习角落</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">262</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">40</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/hellozhaozheng" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:hellozhaozheng@foxmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/ksws0292756" title="零域CSDN博客" target="_blank">零域CSDN博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://xinghanzzy.github.io/" title="BoXiao的博客" target="_blank">BoXiao的博客</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#总目录篇"><span class="nav-text">总目录篇</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习篇"><span class="nav-text">机器学习篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本概念"><span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑回归"><span class="nav-text">逻辑回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量机"><span class="nav-text">支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树"><span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#降维"><span class="nav-text">降维</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类"><span class="nav-text">聚类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习篇"><span class="nav-text">深度学习篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#优化方法"><span class="nav-text">优化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#初始化方法"><span class="nav-text">初始化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化"><span class="nav-text">归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#感受野"><span class="nav-text">感受野</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#全连接层"><span class="nav-text">全连接层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积层"><span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反卷积层"><span class="nav-text">反卷积层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#池化层"><span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练问题"><span class="nav-text">训练问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#网络结构篇"><span class="nav-text">网络结构篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGGNet"><span class="nav-text">VGGNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InceptionV1"><span class="nav-text">InceptionV1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InceptionV3"><span class="nav-text">InceptionV3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-InceptionV2-相比于-GoogLeNet-有什么区别"><span class="nav-text">简述 InceptionV2 相比于 GoogLeNet 有什么区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-InceptionV3-相比于-GoogLeNet-有什么区别"><span class="nav-text">简述 InceptionV3 相比于 GoogLeNet 有什么区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InceptionV4-and-Inception-ResNet"><span class="nav-text">InceptionV4 and Inception ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-InceptionV4-做了哪些改进"><span class="nav-text">简述 InceptionV4 做了哪些改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-Inception-Resnet-v1-做了哪些改进"><span class="nav-text">简述 Inception-Resnet-v1 做了哪些改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-Inception-ResNet-v2-做了哪些改进"><span class="nav-text">简述 Inception-ResNet-v2 做了哪些改进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Xception"><span class="nav-text">Xception</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNeXt"><span class="nav-text">ResNeXt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DenseNet"><span class="nav-text">DenseNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SqueezeNet"><span class="nav-text">SqueezeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MobileNet"><span class="nav-text">MobileNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MobileNetV2"><span class="nav-text">MobileNetV2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ShuffleNet"><span class="nav-text">ShuffleNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ShuffleNetV2"><span class="nav-text">ShuffleNetV2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SENet"><span class="nav-text">SENet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#目标检测篇"><span class="nav-text">目标检测篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#NMS"><span class="nav-text">NMS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#R-CNN"><span class="nav-text">R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-R-CNN"><span class="nav-text">Fast R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Faster-R-CNN"><span class="nav-text">Faster R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mask-R-CNN"><span class="nav-text">Mask R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FPN"><span class="nav-text">FPN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RetinaNet"><span class="nav-text">RetinaNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SSD"><span class="nav-text">SSD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLO"><span class="nav-text">YOLO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#YOLOv1"><span class="nav-text">YOLOv1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YOLOv2"><span class="nav-text">YOLOv2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YOLOv3"><span class="nav-text">YOLOv3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他"><span class="nav-text">其他</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#图像处理篇"><span class="nav-text">图像处理篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#图像放缩"><span class="nav-text">图像放缩</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#边缘检测算法"><span class="nav-text">边缘检测算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#霍夫变换"><span class="nav-text">霍夫变换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像保边滤波器"><span class="nav-text">图像保边滤波器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像平移"><span class="nav-text">图像平移</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像开操作、闭操作"><span class="nav-text">图像开操作、闭操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像旋转"><span class="nav-text">图像旋转</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像重建质量评价指标"><span class="nav-text">图像重建质量评价指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#光流法"><span class="nav-text">光流法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像去噪的方法"><span class="nav-text">图像去噪的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#度量图像patch相似度的方法"><span class="nav-text">度量图像patch相似度的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#传统图像处理CDC做过吗？"><span class="nav-text">传统图像处理CDC做过吗？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#傅里叶变换"><span class="nav-text">傅里叶变换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像融合算法有哪些？"><span class="nav-text">图像融合算法有哪些？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像增强算法有哪些"><span class="nav-text">图像增强算法有哪些</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像滤波方法"><span class="nav-text">图像滤波方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#直方图均衡化"><span class="nav-text">直方图均衡化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数学基础篇"><span class="nav-text">数学基础篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概率分布"><span class="nav-text">概率分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性代数"><span class="nav-text">线性代数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#常见问题篇"><span class="nav-text">常见问题篇</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#未整理的问题"><span class="nav-text">未整理的问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#逻辑回归-1"><span class="nav-text">逻辑回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性回归和逻辑回归的原理和区别"><span class="nav-text">线性回归和逻辑回归的原理和区别</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#在Faster-RCNN中-如果两个物体重合度很高-会怎么样"><span class="nav-text">在Faster RCNN中, 如果两个物体重合度很高, 会怎么样</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络复杂度分析"><span class="nav-text">卷积神经网络复杂度分析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积计算-卷积层特参数个数及征图谱尺寸计算"><span class="nav-text">卷积计算,卷积层特参数个数及征图谱尺寸计算</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#L2正则化和L2规范化-归一化-的不同"><span class="nav-text">L2正则化和L2规范化(归一化)的不同</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#从-rcnn-到-faster"><span class="nav-text">从 rcnn 到 faster</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#为什么fast-rcnn的roi-pooling比spp的-spatial-pooling效果好"><span class="nav-text">为什么fast rcnn的roi pooling比spp的 spatial pooling效果好???</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络参数初始化"><span class="nav-text">神经网络参数初始化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分类问题为什么用交叉熵"><span class="nav-text">分类问题为什么用交叉熵</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Relu和Dropout都具有正则化作用-它们在正则化方面的区别是什么"><span class="nav-text">Relu和Dropout都具有正则化作用, 它们在正则化方面的区别是什么?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#介绍一下-hard-negative-mining-难样例挖掘"><span class="nav-text">介绍一下 hard negative mining(难样例挖掘)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dropout内部是怎么实现的"><span class="nav-text">dropout内部是怎么实现的</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#简述一下BN"><span class="nav-text">简述一下BN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#各种初始化方式，及公式各个参数对训练的影响"><span class="nav-text">各种初始化方式，及公式各个参数对训练的影响</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#目标检测，数据不平衡问题怎么解决"><span class="nav-text">目标检测，数据不平衡问题怎么解决</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#你的zerotensor和TF比性能上有优势吗"><span class="nav-text">你的zerotensor和TF比性能上有优势吗</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#常用的数据增强技术"><span class="nav-text">常用的数据增强技术</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#有哪些可以避免过拟合的办法"><span class="nav-text">有哪些可以避免过拟合的办法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#正则化L1和L2的区别"><span class="nav-text">正则化L1和L2的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#rcnn。。"><span class="nav-text">rcnn。。</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推导svm"><span class="nav-text">推导svm</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积层的参数个数计算公式是："><span class="nav-text">卷积层的参数个数计算公式是：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度消失和梯度爆炸的概念，引用这两个现象的原因及其解决办法"><span class="nav-text">梯度消失和梯度爆炸的概念，引用这两个现象的原因及其解决办法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#关于各种激活函数的解析与讨论"><span class="nav-text">关于各种激活函数的解析与讨论</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#简述ResNet"><span class="nav-text">简述ResNet</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#嵌入式开发很底层-一般还是倾向于做一些上层的东西"><span class="nav-text">嵌入式开发很底层   一般还是倾向于做一些上层的东西</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推导SVM"><span class="nav-text">推导SVM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#比较Boosting和Bagging的异同"><span class="nav-text">比较Boosting和Bagging的异同</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#无监督学习中存在过拟合吗"><span class="nav-text">无监督学习中存在过拟合吗?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#什么是K折交叉验证"><span class="nav-text">什么是K折交叉验证?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#关于k折交叉验证-需要注意什么"><span class="nav-text">关于k折交叉验证, 需要注意什么?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对于一个二分类问题-我们定义超过阈值t的判定为正例-否则判定为负例-现在若将t增大-则准确率和召回率会如何变化"><span class="nav-text">对于一个二分类问题, 我们定义超过阈值t的判定为正例, 否则判定为负例. 现在若将t增大, 则准确率和召回率会如何变化?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#增加网络层数-是否总能减小训练集错误率"><span class="nav-text">增加网络层数, 是否总能减小训练集错误率?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#在目标检测问题上-如何做数据增广"><span class="nav-text">在目标检测问题上, 如何做数据增广?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#softmax怎么跟交叉熵损失函数结合"><span class="nav-text">softmax怎么跟交叉熵损失函数结合?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#用梯度下降训练神经网络的参数-为什么参数有时候会被训练为nan值"><span class="nav-text">用梯度下降训练神经网络的参数, 为什么参数有时候会被训练为nan值?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#有没有自己试过更改模型的框架"><span class="nav-text">有没有自己试过更改模型的框架.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#说一下你所有的提高精度的方法-并且说明它们带来了多少的精度提升"><span class="nav-text">说一下你所有的提高精度的方法, 并且说明它们带来了多少的精度提升!</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SSD已经使用了难样例挖掘的技巧-Focal-Loss-相比之下为什么能够提高"><span class="nav-text">SSD已经使用了难样例挖掘的技巧, Focal Loss 相比之下为什么能够提高</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#池化的优点-优化的缺点"><span class="nav-text">池化的优点, 优化的缺点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#待定"><span class="nav-text">待定</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#完善bisai待看"><span class="nav-text">完善bisai待看</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#谈谈你参加的比赛"><span class="nav-text">谈谈你参加的比赛</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对于一个新任务-你一般都会使用那些数据预处理方法"><span class="nav-text">(对于一个新任务,) 你一般都会使用那些数据预处理方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#比赛中用到的模型融合方法"><span class="nav-text">比赛中用到的模型融合方法:</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#你的方法与其他人方法的区别是什么-为什么比别人的方法差"><span class="nav-text">你的方法与其他人方法的区别是什么? 为什么比别人的方法差?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对于faster-rcnn-你都调了哪些参数"><span class="nav-text">对于faster rcnn 你都调了哪些参数?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BN具体是什么实现的"><span class="nav-text">BN具体是什么实现的</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对于平均移动了解吗"><span class="nav-text">对于平均移动了解吗</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#积分图-快速求矩阵的核"><span class="nav-text">积分图, 快速求矩阵的核</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#样本不均衡问题怎么解决"><span class="nav-text">样本不均衡问题怎么解决</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#详细说一下Focal-Loss"><span class="nav-text">详细说一下Focal Loss</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#说一下为什么Faster-比YOLO和SSD更准确"><span class="nav-text">说一下为什么Faster 比YOLO和SSD更准确</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#样本采样的理论化值是"><span class="nav-text">样本采样的理论化值是</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#anchor的参数设值怎么选的？-为什么这么设置"><span class="nav-text">anchor的参数设值怎么选的？  为什么这么设置</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#在调试RPN网络时有没有遇到什么问题？"><span class="nav-text">在调试RPN网络时有没有遇到什么问题？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#简述一下faster-rcnn模型"><span class="nav-text">简述一下faster rcnn模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#简述一下ResNet模型及它解决的问题"><span class="nav-text">简述一下ResNet模型及它解决的问题</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZeroZone</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">2.4m</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
    <span title="站点阅读时长">35:45</span>
  
</div>










  <div class="footer-custom">勤练带来力量</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
